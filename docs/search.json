[
  {
    "objectID": "Regression_noncaus_chapter_jul25.html",
    "href": "Regression_noncaus_chapter_jul25.html",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "",
    "text": "Replication materials for: Inglese, Mazzola, Goria, Ferrarotti & Cornillie, “Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian” in Inglese, Mazzola & Goria Diachronic and Typological Perspectives on Anticausativization (Accepted December 2025). Full citation information will be released after publication."
  },
  {
    "objectID": "Regression_noncaus_chapter_jul25.html#load-and-prepare-data",
    "href": "Regression_noncaus_chapter_jul25.html#load-and-prepare-data",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "Load and Prepare Data",
    "text": "Load and Prepare Data\n\n# List of packages used in this document\npkgs &lt;- c(\"tidyverse\", \"lme4\", \"kableExtra\", \"readxl\", \n          \"Boruta\", \"ranger\", \"vip\", \"dplyr\", \n          \"purrr\", \"stringr\", \"ggpattern\")\n\n# Install any missing packages\nnew_pkgs &lt;- pkgs[!(pkgs %in% installed.packages()[, \"Package\"])]\nif(length(new_pkgs)) install.packages(new_pkgs)\n\n# Load all packages\nlapply(pkgs, library, character.only = TRUE)\n\n\ndata_all &lt;- read_excel(\"romall_new_VNC_pub.xlsx\")\ndata &lt;- read_excel(\"Inglese_et_al_noncaus_july25_newVNC_pub.xlsx\")\n\nData preparation, summary and NAs check:\n\ndata$period &lt;- data$vnc_period_apr25\ndata$caus_use &lt;- data$caus_use_jul25\n\nsummarydata &lt;- fct_count(data$coding)\nlanguagesummary &lt;- fct_count(data$language)\n\nvars_to_check &lt;- c(\"coding\", \"telicity\", \"animacy\", \"finiteness\", \"genre\", \"control\", \"language\", \n                   \"reflpriming\", \"caus_use\", \"year\")\n\nany_na_rows &lt;- data[!complete.cases(data[, vars_to_check]), ]\nn_na &lt;- nrow(any_na_rows)\ncat(\"Number of rows with NA in model variables:\", n_na, \"\\n\")\n\nNumber of rows with NA in model variables: 0 \n\nna_counts &lt;- colSums(is.na(data[, vars_to_check]))\nna_counts[na_counts &gt; 0]\n\nnamed numeric(0)\n\ndata &lt;- data %&gt;% mutate(across(\n  c(coding, telicity, animacy, finiteness, compound_tense, genre, time, \n    control, aspect, language, mood, subjcoding, reflpriming), \n  ~factor(.x)))\n\nlevels(data$language) &lt;- str_to_sentence(levels(data$language))"
  },
  {
    "objectID": "Regression_noncaus_chapter_jul25.html#rationale",
    "href": "Regression_noncaus_chapter_jul25.html#rationale",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "Rationale",
    "text": "Rationale\n\n\n\n\n\n\nObject of the study\n\n\n\nA comparative diachronic corpus study of Italian and Spanish focuses on the alternation between anticausativization (reflexive marking) and lability as noncausal marking strategies.\n\n\n\n\n\n\n\n\nResearch Questions\n\n\n\n\nWhat are the factors that influence the choice between the anticausative and the labile strategies in Italian and Spanish?\n\nDo these factors change over time?\n\nDo the strength and relevance of these factors change when Italian and Spanish are compared?\n\n\n\n\n\n\n\n\n\nData\n\n\n\n\nItalian: MIDIA corpus (D’Achille & Grossmann 2017), ~7.8 million words, balanced across tokens and genres (13th–mid-20th centuries).\n\nSpanish: Corpus del Diccionario Histórico del Español (CDH, Real Academia Española 2013), Peninsular Spanish only.\nWe extracted a sample of occurrences for the Italian and Spanish equivalents of the 20 verb meaning pairs listed in Haspelmath et al. (2014). See paper for the complete procedure.\n\n\n\nThe complete dataset includes causal and noncausal uses of the verbs extracted, contained in the dataset data_all. For this study we only include noncausal observations and only verbs with variability (data). We therefore removed the verbs with categorical selection of wither anticausative or labile marking. Figure 2 and 3 show the lemmas per language and the proportion of anticausative vs. labile marking, before filtering to only include the vairable contexts.\nFigure 1:\n\n# ---- Italian data ----\nitalian_data &lt;- data_all %&gt;%\n  filter(semantics == \"noncaus\", language == \"italian\") %&gt;%\n  group_by(lemma, coding) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  complete(lemma, coding = c(\"antic\", \"zero\"), fill = list(n = 0)) %&gt;%\n  group_by(lemma) %&gt;%\n  mutate(perc = n / sum(n) * 100) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    coding_label = recode(coding, \"antic\" = \"Anticausative\", \"zero\" = \"Labile\"),\n    coding_label = factor(coding_label, levels = c(\"Anticausative\", \"Labile\"))\n  )\n\n# Order lemmas by Anticausative %\nitalian_order &lt;- italian_data %&gt;%\n  filter(coding_label == \"Anticausative\") %&gt;%\n  arrange(desc(perc)) %&gt;%\n  pull(lemma)\n\nitalian_data &lt;- italian_data %&gt;%\n  mutate(lemma = factor(lemma, levels = italian_order))\n\n# ---- Plot Italian ----\nplot_italian &lt;- ggplot(italian_data, aes(x = lemma, y = perc, fill = coding_label)) +\n  geom_col(position = \"stack\") +\n  scale_fill_manual(values = c(\"Anticausative\" = \"grey30\", \"Labile\" = \"grey80\")) +\n  labs(x = \"Verbs\", y = \"Percentage\", fill = \"Patterns\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n# ---- Print plots ----\nplot_italian\n\n\n\n\n\n\n\n\nFigure 2:\n\n# ---- Spanish data ----\nspanish_data &lt;- data_all %&gt;%\n  filter(semantics == \"noncaus\", language == \"spanish\") %&gt;%\n  group_by(lemma, coding) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  complete(lemma, coding = c(\"antic\", \"zero\"), fill = list(n = 0)) %&gt;%\n  group_by(lemma) %&gt;%\n  mutate(perc = n / sum(n) * 100) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    coding_label = recode(coding, \"antic\" = \"Anticausative\", \"zero\" = \"Labile\"),\n    coding_label = factor(coding_label, levels = c(\"Anticausative\", \"Labile\"))\n  )\n\n\n# Order lemmas by Anticausative %\nspanish_order &lt;- spanish_data %&gt;%\n  filter(coding_label == \"Anticausative\") %&gt;%\n  arrange(desc(perc)) %&gt;%\n  pull(lemma)\n\nspanish_data &lt;- spanish_data %&gt;%\n  mutate(lemma = factor(lemma, levels = spanish_order))\n\n# ---- Plot Spanish ----\nplot_spanish &lt;- ggplot(spanish_data, aes(x = lemma, y = perc, fill = coding_label)) +\n  geom_col(position = \"stack\") +\n  scale_fill_manual(values = c(\"Anticausative\" = \"grey30\", \"Labile\" = \"grey80\")) +\n  labs(x = \"Verbs\", y = \"Percentage\", fill = \"Patterns\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\nplot_spanish\n\n\n\n\n\n\n\n\nThe filtered dataset used for this study is contained in data and includes 5154 noncausal observations. The variables used in the following statistical analysis were manually annotated and are distributed as reported in the table below (Table 1 in the paper):\n\nvars_to_check &lt;- c(\n  \"coding\", \"telicity\", \"animacy\", \"finiteness\", \"genre\",\n  \"control\", \"language\", \"reflpriming\"\n)\n\n# ---- Factor variables ----\ntable_factors &lt;- map_dfr(vars_to_check, function(v) {\n\n  data %&gt;%\n    count(.data[[v]]) %&gt;%\n    mutate(\n      Variables = str_to_sentence(v),\n      Values = str_to_sentence(as.character(.data[[v]])),\n      `N. occurrences` = n\n    ) %&gt;%\n    select(Variables, Values, `N. occurrences`)\n})\n\n# ---- Numeric variables (manual add) ----\ntable_numeric &lt;- tibble(\n  Variables = str_to_sentence(c(\"caus_use\", \"year\")),\n  Values = c(\n    paste0(min(data$caus_use, na.rm = TRUE), \" – \",\n           max(data$caus_use, na.rm = TRUE)),\n    paste0(min(data$year, na.rm = TRUE), \" – \",\n           max(data$year, na.rm = TRUE))\n  ),\n  `N. occurrences` = NA_integer_\n)\n\n# ---- Combine & print ----\nbind_rows(table_factors, table_numeric) %&gt;%\n  kbl(\n    booktabs = TRUE,\n    align = \"l\",\n    caption = \"Distribution of variables\"\n  ) %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    bootstrap_options = c(\"striped\", \"hover\")\n  ) %&gt;%\n  collapse_rows(\n    columns = 1,\n    valign = \"top\"\n  )\n\n\nDistribution of variables\n\n\nVariables\nValues\nN. occurrences\n\n\n\n\nCoding\nAntic\n3209\n\n\nZero\n1945\n\n\nTelicity\nAtelic\n1612\n\n\nTelic\n3542\n\n\nAnimacy\nNo\n2783\n\n\nYes\n2371\n\n\nFiniteness\nFin\n4055\n\n\nNonfin\n1099\n\n\nGenre\nPoesia\n997\n\n\nProsa\n3782\n\n\nTeatro\n375\n\n\nControl\nNo\n3367\n\n\nYes\n1787\n\n\nLanguage\nItalian\n2468\n\n\nSpanish\n2686\n\n\nReflpriming\nNo\n4648\n\n\nYes\n506\n\n\nCaus_use\n0 – 0.939393939393939\nNA\n\n\nYear\n1140 – 2001\nNA"
  },
  {
    "objectID": "Regression_noncaus_chapter_jul25.html#random-forest-for-variable-importance",
    "href": "Regression_noncaus_chapter_jul25.html#random-forest-for-variable-importance",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "Random Forest for Variable Importance",
    "text": "Random Forest for Variable Importance\nAs a first step in exploring our data, we fitted a random forest (RF) model to inspect the relative importance of the predictors. RF are a type of decision tree ensemble model that can be used to detect patterns in the data and assess which variables are most strongly associated with the outcome (Levshina 2020), and help us making decisions about the regression model.\n\nset.seed(123)\nrf1 &lt;- ranger(coding ~ year + caus_use_jul25 + \n                telicity + animacy + finiteness + compound_tense + \n                genre + time + control + aspect + language + mood +  \n                subjcoding + reflpriming,\n              data = data, importance = \"impurity_corrected\")\n\nFigure 3:\n\nrflabels&lt;- c( \"Compound tense\",\"Tense\", \"Mood\", \"Reflexive Priming\", \"Aspect\",  \"Subject Coding\", \"Finiteness\", \"Language\", \"Genre\", \"Animacy\", \"Year\", \"Control\", \"Causalness degree\", \"Telicity\")\n\nrf_plot&lt;-vip(rf1, num_features = 15) + scale_x_discrete(labels = rflabels)\n\nrf_plot\n\n\n\n\n\n\n\n\nRandom Forest - Variables imporance:\n\nvip:::vi(rf1) %&gt;% \n  mutate_if(is.numeric, ~ round(.x, 2)) %&gt;%\n  kbl() %&gt;% \n  kable_styling()\n\n\n\n\nVariable\nImportance\n\n\n\n\ntelicity\n650.30\n\n\ncaus_use_jul25\n321.60\n\n\ncontrol\n86.22\n\n\nyear\n47.00\n\n\nanimacy\n28.38\n\n\ngenre\n21.84\n\n\nlanguage\n16.64\n\n\naspect\n8.36\n\n\nfiniteness\n7.91\n\n\nsubjcoding\n7.13\n\n\nreflpriming\n6.75\n\n\ntime\n3.11\n\n\nmood\n2.27\n\n\ncompound_tense\n0.70\n\n\n\n\n\nRandom Forest - Model diagnostics\n\nrf2 &lt;- ranger(coding ~ year +  caus_use_jul25 + \n                telicity + animacy + finiteness + compound_tense + \n                genre + time + control + aspect + language + mood +  \n                subjcoding + reflpriming,\n              data = data)\n\nrf2_pred_df &lt;- bind_cols(data, .pred = predict(rf2, data)$predictions)\n\ndiagnostics &lt;- Hmisc::somers2(as.numeric(rf2_pred_df$.pred) - 1, \n                               as.numeric(rf2_pred_df$coding) - 1) %&gt;% \n  enframe() %&gt;% \n  mutate(value = round(value, 2))\n\ndiagnostics %&gt;% kbl() %&gt;% kable_styling()\n\n\n\n\nname\nvalue\n\n\n\n\nC\n0.93\n\n\nDxy\n0.86\n\n\nn\n5154.00\n\n\nMissing\n0.00"
  },
  {
    "objectID": "Regression_noncaus_chapter_jul25.html#mixed-effect-logistic-regression",
    "href": "Regression_noncaus_chapter_jul25.html#mixed-effect-logistic-regression",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "Mixed-effect logistic regression",
    "text": "Mixed-effect logistic regression\nA logistic regression model with mixed effects predicts the outcome of a binary variable (in our case, SE vs. lability) given multiple explanatory factors (and their interactions), and also includes control variables, called random effects. These models are useful when the data have a hierarchical or grouped structure, as they allow us to account for variability due to such groupings, —here, verbs (LEMMA) and authors (AUTHOR).\nThe models were calculated using the lme4::glmer function (Bates et al. 2015), fitting a maximal interaction model, i.e., including all interactions between predictors and the variables YEAR or LANGUAGE.\n\nPolynomial\nWe do not assume a linear relation between year and the other predictors, i.e. we appreciate that the effects could fluctuate over time, losing predictive power or direction of the effects. This is why we included YEAR as a polynomial.\nWe fitted a linear, quadratic and a cubic model (only with simple effects). We compared the AIC, BIC and used ANOVA to compare pairs of nested models. All these tests showed that a quadratic polynomial is the best fit for the diachronic development of the dependent variable (SE vs. lability): it improves the fit compared to the linear model and it does not need a cubic term; AIC and BIC for the quadratic models are the lowest.\n\n# Linear\nmod_lin &lt;- glmer(coding ~ telicity + animacy + finiteness + compound_tense +genre +\n                control + language + reflpriming + caus_use + poly(year,1) +\n                (1|lemma) + (1|author),\n                family=\"binomial\", data=data, nAGQ=0)\n# Quadratic\n\n\nmod_quad &lt;- glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n                control + language + reflpriming + caus_use + poly(year,2) +\n                (1|lemma) + (1|author),\n                family=\"binomial\", data=data, nAGQ=0)\n\n# Cubic\n\nmod_cub &lt;- glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n                control + language + reflpriming + caus_use + poly(year,3) +\n                (1|lemma) + (1|author),\n                family=\"binomial\", data=data, nAGQ=0)\n\nAIC(mod_lin, mod_cub, mod_quad)\n\n         df      AIC\nmod_lin  14 2643.264\nmod_cub  16 2642.871\nmod_quad 15 2641.216\n\nBIC(mod_lin, mod_cub, mod_quad)\n\n         df      BIC\nmod_lin  14 2734.929\nmod_cub  16 2747.631\nmod_quad 15 2739.429\n\nanova(mod_lin, mod_quad)\n\nData: data\nModels:\nmod_lin: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 1) + (1 | lemma) + (1 | author)\nmod_quad: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + (1 | lemma) + (1 | author)\n         npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nmod_lin    14 2643.3 2734.9 -1307.6   2615.3                       \nmod_quad   15 2641.2 2739.4 -1305.6   2611.2 4.0478  1    0.04423 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod_cub, mod_quad)\n\nData: data\nModels:\nmod_quad: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + (1 | lemma) + (1 | author)\nmod_cub: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 3) + (1 | lemma) + (1 | author)\n         npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod_quad   15 2641.2 2739.4 -1305.6   2611.2                     \nmod_cub    16 2642.9 2747.6 -1305.4   2610.9 0.3454  1     0.5567\n\n\n\n\nModel selection\nThe most parsimonious interaction model was found by performing a step-wise backward selection procedure, which consists in removing non-significant interactions one by one, starting from the one with the highest p-value.\nFirst we fit the maximal model with all variables and interactions:\n\nmod_all&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n                      control + language + reflpriming + caus_use + poly(year,2) +\n                #year\n                  telicity*poly(year,2) + animacy*poly(year,2) + finiteness*poly(year,2) + compound_tense*poly(year,2) + \n                 genre*poly(year,2) + control*poly(year,2) + language*poly(year,2) + \n                 reflpriming*poly(year,2) + caus_use*poly(year,2) +\n                #language\n                  telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ genre*language+\n                  control*language+ reflpriming*language+ caus_use*language+\n                #random\n                (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\n\nsummary(mod_all)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    finiteness * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + language *  \n    poly(year, 2) + reflpriming * poly(year, 2) + caus_use *  \n    poly(year, 2) + telicity * language + animacy * language +  \n    finiteness * language + compound_tense * language + genre *  \n    language + control * language + reflpriming * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2609.9   2898.0  -1261.0   2521.9     5110 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.7873 -0.2150 -0.1132  0.2094 11.1910 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2164   0.4652  \n lemma  (Intercept) 5.0986   2.2580  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.40499    0.74866   0.541 0.588540    \ntelicitytelic                      -2.19744    0.41309  -5.319 1.04e-07 ***\nanimacyyes                          1.05674    0.37433   2.823 0.004758 ** \nfinitenessnonfin                    0.72955    0.22499   3.243 0.001185 ** \ncompound_tenseyes                   0.95798    0.39661   2.415 0.015718 *  \ngenreProsa                         -0.59627    0.22444  -2.657 0.007891 ** \ngenreTeatro                        -0.35637    0.32793  -1.087 0.277152    \ncontrolyes                         -1.27666    0.44580  -2.864 0.004186 ** \nlanguageSpanish                     0.08508    1.02514   0.083 0.933856    \nreflprimingyes                     -1.31152    0.37642  -3.484 0.000494 ***\ncaus_use                            0.32063    0.83799   0.383 0.701999    \npoly(year, 2)1                     33.39782   16.53895   2.019 0.043452 *  \npoly(year, 2)2                      0.44726   16.08470   0.028 0.977816    \ntelicitytelic:poly(year, 2)1      -24.28686   11.33634  -2.142 0.032162 *  \ntelicitytelic:poly(year, 2)2      -26.95656   10.30288  -2.616 0.008886 ** \nanimacyyes:poly(year, 2)1          -1.28888   11.77304  -0.109 0.912824    \nanimacyyes:poly(year, 2)2         -20.63096   11.24370  -1.835 0.066522 .  \nfinitenessnonfin:poly(year, 2)1    -3.49210   10.77803  -0.324 0.745937    \nfinitenessnonfin:poly(year, 2)2     0.99589   10.21325   0.098 0.922322    \ncompound_tenseyes:poly(year, 2)1  -50.32779   21.42149  -2.349 0.018803 *  \ncompound_tenseyes:poly(year, 2)2  -15.57524   21.04247  -0.740 0.459190    \ngenreProsa:poly(year, 2)1         -27.31368   14.04492  -1.945 0.051807 .  \ngenreTeatro:poly(year, 2)1          9.22838   23.68108   0.390 0.696763    \ngenreProsa:poly(year, 2)2          18.22159   12.75048   1.429 0.152978    \ngenreTeatro:poly(year, 2)2         -4.77803   22.99945  -0.208 0.835428    \ncontrolyes:poly(year, 2)1          26.87501   13.17733   2.039 0.041401 *  \ncontrolyes:poly(year, 2)2          -5.80183   12.27343  -0.473 0.636417    \nlanguageSpanish:poly(year, 2)1      4.64850   11.78480   0.394 0.693250    \nlanguageSpanish:poly(year, 2)2     -0.18386   11.76494  -0.016 0.987531    \nreflprimingyes:poly(year, 2)1       8.44986   15.24258   0.554 0.579333    \nreflprimingyes:poly(year, 2)2       3.33422   14.03653   0.238 0.812239    \ncaus_use:poly(year, 2)1           -23.56069   23.89754  -0.986 0.324180    \ncaus_use:poly(year, 2)2            28.51399   22.17092   1.286 0.198409    \ntelicitytelic:languageSpanish       1.42471    0.50389   2.827 0.004692 ** \nanimacyyes:languageSpanish         -0.25338    0.45267  -0.560 0.575650    \nfinitenessnonfin:languageSpanish   -0.51375    0.28422  -1.808 0.070666 .  \ncompound_tenseyes:languageSpanish  -0.76066    0.58092  -1.309 0.190392    \ngenreProsa:languageSpanish          0.21263    0.34075   0.624 0.532630    \ngenreTeatro:languageSpanish        -0.70863    0.61600  -1.150 0.249996    \ncontrolyes:languageSpanish          0.73903    0.54686   1.351 0.176569    \nlanguageSpanish:reflprimingyes      0.77442    0.46226   1.675 0.093876 .  \nlanguageSpanish:caus_use           -2.53425    1.06053  -2.390 0.016866 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBy inspecting the model summary we procede with the elimination of the first least significant intercation, language*poly(year,2).\n\n# remove language:year\n\nmod1&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + finiteness*poly(year,2) + compound_tense*poly(year,2) + genre*poly(year,2) +\n              control*poly(year,2) + reflpriming*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ genre*language+\n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod1)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    finiteness * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + reflpriming *  \n    poly(year, 2) + caus_use * poly(year, 2) + telicity * language +  \n    animacy * language + finiteness * language + compound_tense *  \n    language + genre * language + control * language + reflpriming *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2606.1   2881.1  -1261.0   2522.1     5112 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.3982 -0.2150 -0.1139  0.2102 11.1985 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2177   0.4666  \n lemma  (Intercept) 5.0961   2.2575  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.39230    0.74661   0.525 0.599277    \ntelicitytelic                      -2.19729    0.41354  -5.313 1.08e-07 ***\nanimacyyes                          1.05367    0.37369   2.820 0.004808 ** \nfinitenessnonfin                    0.72751    0.22425   3.244 0.001178 ** \ncompound_tenseyes                   0.95198    0.39615   2.403 0.016257 *  \ngenreProsa                         -0.59575    0.22425  -2.657 0.007892 ** \ngenreTeatro                        -0.35021    0.32739  -1.070 0.284762    \ncontrolyes                         -1.28913    0.44443  -2.901 0.003724 ** \nlanguageSpanish                     0.07796    1.02413   0.076 0.939318    \nreflprimingyes                     -1.31249    0.37632  -3.488 0.000487 ***\ncaus_use                            0.34969    0.83135   0.421 0.674023    \npoly(year, 2)1                     33.93167   16.48071   2.059 0.039507 *  \npoly(year, 2)2                     -0.21185   15.44538  -0.014 0.989056    \ntelicitytelic:poly(year, 2)1      -24.40141   11.33997  -2.152 0.031413 *  \ntelicitytelic:poly(year, 2)2      -26.75610   10.28271  -2.602 0.009267 ** \nanimacyyes:poly(year, 2)1          -0.75738   11.70432  -0.065 0.948406    \nanimacyyes:poly(year, 2)2         -20.71886   11.15169  -1.858 0.063181 .  \nfinitenessnonfin:poly(year, 2)1    -3.72487   10.77288  -0.346 0.729520    \nfinitenessnonfin:poly(year, 2)2     1.14890   10.21358   0.112 0.910437    \ncompound_tenseyes:poly(year, 2)1  -50.65284   21.40967  -2.366 0.017987 *  \ncompound_tenseyes:poly(year, 2)2  -15.84424   21.05255  -0.753 0.451688    \ngenreProsa:poly(year, 2)1         -25.49010   13.06657  -1.951 0.051082 .  \ngenreTeatro:poly(year, 2)1          9.31678   23.45156   0.397 0.691163    \ngenreProsa:poly(year, 2)2          18.91926   11.80235   1.603 0.108933    \ngenreTeatro:poly(year, 2)2         -3.43107   22.74751  -0.151 0.880108    \ncontrolyes:poly(year, 2)1          26.81281   13.18044   2.034 0.041923 *  \ncontrolyes:poly(year, 2)2          -6.01864   12.26139  -0.491 0.623524    \nreflprimingyes:poly(year, 2)1       8.74942   15.21229   0.575 0.565187    \nreflprimingyes:poly(year, 2)2       3.16663   14.01661   0.226 0.821264    \ncaus_use:poly(year, 2)1           -22.37107   23.66925  -0.945 0.344580    \ncaus_use:poly(year, 2)2            28.64179   22.13466   1.294 0.195673    \ntelicitytelic:languageSpanish       1.42524    0.50422   2.827 0.004705 ** \nanimacyyes:languageSpanish         -0.24983    0.45207  -0.553 0.580510    \nfinitenessnonfin:languageSpanish   -0.50645    0.28289  -1.790 0.073403 .  \ncompound_tenseyes:languageSpanish  -0.74728    0.57946  -1.290 0.197188    \ngenreProsa:languageSpanish          0.23142    0.33402   0.693 0.488414    \ngenreTeatro:languageSpanish        -0.67788    0.60970  -1.112 0.266211    \ncontrolyes:languageSpanish          0.74910    0.54593   1.372 0.170013    \nlanguageSpanish:reflprimingyes      0.77459    0.46234   1.675 0.093860 .  \nlanguageSpanish:caus_use           -2.55745    1.04522  -2.447 0.014413 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod_all, mod1)\n\nData: data\nModels:\nmod1: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + reflpriming * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming *      language + caus_use * language + (1 | lemma) + (1 | author)\nmod_all: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + language * poly(year, 2) + reflpriming * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control *      language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n        npar    AIC    BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod1      42 2606.1 2881.1  -1261   2522.1                     \nmod_all   44 2609.9 2898.0  -1261   2521.9 0.1625  2      0.922\n\n\nThe ANOVA comparison shows that mod1 is better than mod_all, as removing the interaction does not significantly affect the model fit (the p-value is below significance level). We proceed by removing the least significant interactions terms one-by-one and compare with ANOVA, until we reach the final model, model12.\nRemove reflpriming:poly(year, 2)\n\n# remove reflprimingyes:poly(year, 2)\n\nmod2&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + finiteness*poly(year,2) + compound_tense*poly(year,2) + genre*poly(year,2) +\n              control*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ genre*language+\n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod2)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    finiteness * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use *  \n    poly(year, 2) + telicity * language + animacy * language +  \n    finiteness * language + compound_tense * language + genre *  \n    language + control * language + reflpriming * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2602.4   2864.3  -1261.2   2522.4     5114 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.3099 -0.2148 -0.1138  0.2103 10.5369 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2174   0.4663  \n lemma  (Intercept) 5.1010   2.2585  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.39195    0.74660   0.525 0.599599    \ntelicitytelic                      -2.19692    0.41351  -5.313 1.08e-07 ***\nanimacyyes                          1.05213    0.37349   2.817 0.004848 ** \nfinitenessnonfin                    0.73052    0.22406   3.260 0.001113 ** \ncompound_tenseyes                   0.94581    0.39615   2.388 0.016963 *  \ngenreProsa                         -0.59102    0.22395  -2.639 0.008312 ** \ngenreTeatro                        -0.34893    0.32808  -1.064 0.287538    \ncontrolyes                         -1.28918    0.44436  -2.901 0.003717 ** \nlanguageSpanish                     0.09488    1.02280   0.093 0.926091    \nreflprimingyes                     -1.32751    0.37597  -3.531 0.000414 ***\ncaus_use                            0.34579    0.83091   0.416 0.677294    \npoly(year, 2)1                     34.99408   16.39719   2.134 0.032830 *  \npoly(year, 2)2                      0.43033   15.23893   0.028 0.977471    \ntelicitytelic:poly(year, 2)1      -24.26891   11.32692  -2.143 0.032146 *  \ntelicitytelic:poly(year, 2)2      -26.95003   10.23544  -2.633 0.008463 ** \nanimacyyes:poly(year, 2)1          -0.69241   11.60801  -0.060 0.952435    \nanimacyyes:poly(year, 2)2         -21.17860   11.04972  -1.917 0.055281 .  \nfinitenessnonfin:poly(year, 2)1    -3.61152   10.77201  -0.335 0.737422    \nfinitenessnonfin:poly(year, 2)2     1.12707   10.20334   0.110 0.912044    \ncompound_tenseyes:poly(year, 2)1  -50.97581   21.38844  -2.383 0.017157 *  \ncompound_tenseyes:poly(year, 2)2  -15.53787   21.03734  -0.739 0.460159    \ngenreProsa:poly(year, 2)1         -25.45090   13.05124  -1.950 0.051167 .  \ngenreTeatro:poly(year, 2)1          9.43822   23.50149   0.402 0.687978    \ngenreProsa:poly(year, 2)2          19.16725   11.71397   1.636 0.101783    \ngenreTeatro:poly(year, 2)2         -3.53216   22.72296  -0.155 0.876471    \ncontrolyes:poly(year, 2)1          26.48886   13.09906   2.022 0.043156 *  \ncontrolyes:poly(year, 2)2          -5.82345   12.17452  -0.478 0.632415    \ncaus_use:poly(year, 2)1           -23.27363   23.60731  -0.986 0.324199    \ncaus_use:poly(year, 2)2            27.78972   22.08722   1.258 0.208326    \ntelicitytelic:languageSpanish       1.42182    0.50415   2.820 0.004799 ** \nanimacyyes:languageSpanish         -0.25458    0.45191  -0.563 0.573209    \nfinitenessnonfin:languageSpanish   -0.50993    0.28272  -1.804 0.071287 .  \ncompound_tenseyes:languageSpanish  -0.73725    0.57931  -1.273 0.203144    \ngenreProsa:languageSpanish          0.22742    0.33374   0.681 0.495596    \ngenreTeatro:languageSpanish        -0.68064    0.61002  -1.116 0.264522    \ncontrolyes:languageSpanish          0.75613    0.54580   1.385 0.165942    \nlanguageSpanish:reflprimingyes      0.78005    0.45937   1.698 0.089489 .  \nlanguageSpanish:caus_use           -2.57990    1.03880  -2.484 0.013008 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod2, mod1)\n\nData: data\nModels:\nmod2: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming * language + caus_use * language +      (1 | lemma) + (1 | author)\nmod1: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + reflpriming * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming *      language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod2   40 2602.4 2864.3 -1261.2   2522.4                     \nmod1   42 2606.1 2881.1 -1261.0   2522.1 0.3341  2     0.8462\n\n\nRemove finiteness:poly(year, 2)\n\n#finitenessnonfin:poly(year, 2)\n\nmod3&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ genre*language+\n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod3)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + caus_use * poly(year, 2) + telicity *  \n    language + animacy * language + finiteness * language + compound_tense *  \n    language + genre * language + control * language + reflpriming *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2598.5   2847.3  -1261.3   2522.5     5116 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.4888 -0.2150 -0.1140  0.2108 10.4773 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2158   0.4645  \n lemma  (Intercept) 5.1019   2.2587  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.3897     0.7465   0.522 0.601650    \ntelicitytelic                      -2.2000     0.4131  -5.325 1.01e-07 ***\nanimacyyes                          1.0553     0.3733   2.827 0.004699 ** \nfinitenessnonfin                    0.7240     0.2196   3.297 0.000978 ***\ncompound_tenseyes                   0.9458     0.3958   2.390 0.016869 *  \ngenreProsa                         -0.5931     0.2238  -2.650 0.008043 ** \ngenreTeatro                        -0.3449     0.3275  -1.053 0.292362    \ncontrolyes                         -1.2911     0.4442  -2.907 0.003652 ** \nlanguageSpanish                     0.1012     1.0223   0.099 0.921168    \nreflprimingyes                     -1.3284     0.3755  -3.538 0.000403 ***\ncaus_use                            0.3539     0.8302   0.426 0.669924    \npoly(year, 2)1                     34.3128    16.2423   2.113 0.034639 *  \npoly(year, 2)2                      0.5570    15.0018   0.037 0.970384    \ntelicitytelic:poly(year, 2)1      -23.8718    11.2669  -2.119 0.034112 *  \ntelicitytelic:poly(year, 2)2      -27.1609    10.1868  -2.666 0.007669 ** \nanimacyyes:poly(year, 2)1          -1.0686    11.5463  -0.093 0.926259    \nanimacyyes:poly(year, 2)2         -21.0320    11.0261  -1.907 0.056459 .  \ncompound_tenseyes:poly(year, 2)1  -50.6192    21.3407  -2.372 0.017694 *  \ncompound_tenseyes:poly(year, 2)2  -15.5197    20.9565  -0.741 0.458958    \ngenreProsa:poly(year, 2)1         -25.3404    13.0299  -1.945 0.051801 .  \ngenreTeatro:poly(year, 2)1          9.0890    23.4572   0.387 0.698407    \ngenreProsa:poly(year, 2)2          18.9435    11.6804   1.622 0.104842    \ngenreTeatro:poly(year, 2)2         -3.2956    22.6719  -0.145 0.884427    \ncontrolyes:poly(year, 2)1          26.3392    13.0922   2.012 0.044239 *  \ncontrolyes:poly(year, 2)2          -5.8408    12.1515  -0.481 0.630756    \ncaus_use:poly(year, 2)1           -23.3948    23.5607  -0.993 0.320732    \ncaus_use:poly(year, 2)2            28.1115    22.0345   1.276 0.202029    \ntelicitytelic:languageSpanish       1.4244     0.5039   2.827 0.004701 ** \nanimacyyes:languageSpanish         -0.2559     0.4518  -0.566 0.571190    \nfinitenessnonfin:languageSpanish   -0.5070     0.2811  -1.803 0.071338 .  \ncompound_tenseyes:languageSpanish  -0.7335     0.5787  -1.267 0.205017    \ngenreProsa:languageSpanish          0.2259     0.3335   0.677 0.498144    \ngenreTeatro:languageSpanish        -0.6852     0.6098  -1.124 0.261139    \ncontrolyes:languageSpanish          0.7568     0.5455   1.387 0.165320    \nlanguageSpanish:reflprimingyes      0.7797     0.4588   1.699 0.089247 .  \nlanguageSpanish:caus_use           -2.5936     1.0372  -2.501 0.012397 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod2, mod3)\n\nData: data\nModels:\nmod3: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod2: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming * language + caus_use * language +      (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod3   38 2598.5 2847.3 -1261.3   2522.5                     \nmod2   40 2602.4 2864.3 -1261.2   2522.4 0.1194  2      0.942\n\n\nRemove genre:language\n\n#genre:language\n\nmod4&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ \n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod4)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + caus_use * poly(year, 2) + telicity *  \n    language + animacy * language + finiteness * language + compound_tense *  \n    language + control * language + reflpriming * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2597.4   2833.1  -1262.7   2525.4     5118 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.9980 -0.2166 -0.1154  0.2084 10.3774 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2122   0.4606  \n lemma  (Intercept) 5.1300   2.2650  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.35409    0.74055   0.478 0.632545    \ntelicitytelic                      -2.19375    0.41264  -5.316 1.06e-07 ***\nanimacyyes                          1.06842    0.37199   2.872 0.004077 ** \nfinitenessnonfin                    0.71006    0.21862   3.248 0.001162 ** \ncompound_tenseyes                   0.94504    0.39507   2.392 0.016753 *  \ngenreProsa                         -0.47683    0.17406  -2.739 0.006155 ** \ngenreTeatro                        -0.51273    0.28431  -1.803 0.071328 .  \ncontrolyes                         -1.28346    0.44229  -2.902 0.003710 ** \nlanguageSpanish                     0.19869    0.98468   0.202 0.840087    \nreflprimingyes                     -1.33386    0.37554  -3.552 0.000382 ***\ncaus_use                            0.32847    0.82890   0.396 0.691901    \npoly(year, 2)1                     34.68193   16.16483   2.146 0.031911 *  \npoly(year, 2)2                      1.59954   14.84409   0.108 0.914189    \ntelicitytelic:poly(year, 2)1      -24.26644   11.24722  -2.158 0.030963 *  \ntelicitytelic:poly(year, 2)2      -27.65528   10.18061  -2.716 0.006598 ** \nanimacyyes:poly(year, 2)1          -0.02539   11.45594  -0.002 0.998232    \nanimacyyes:poly(year, 2)2         -20.85659   10.97189  -1.901 0.057314 .  \ncompound_tenseyes:poly(year, 2)1  -49.52388   21.36581  -2.318 0.020455 *  \ncompound_tenseyes:poly(year, 2)2  -13.14826   20.97386  -0.627 0.530733    \ngenreProsa:poly(year, 2)1         -25.82030   12.91071  -2.000 0.045510 *  \ngenreTeatro:poly(year, 2)1          3.15715   23.34743   0.135 0.892434    \ngenreProsa:poly(year, 2)2          18.19720   11.47024   1.586 0.112633    \ngenreTeatro:poly(year, 2)2        -11.42270   22.23109  -0.514 0.607380    \ncontrolyes:poly(year, 2)1          25.41198   12.97540   1.958 0.050174 .  \ncontrolyes:poly(year, 2)2          -5.74013   12.08826  -0.475 0.634893    \ncaus_use:poly(year, 2)1           -23.14298   23.51364  -0.984 0.324999    \ncaus_use:poly(year, 2)2            28.13574   22.02434   1.277 0.201431    \ntelicitytelic:languageSpanish       1.45617    0.50329   2.893 0.003812 ** \nanimacyyes:languageSpanish         -0.28837    0.45039  -0.640 0.521997    \nfinitenessnonfin:languageSpanish   -0.48929    0.28033  -1.745 0.080914 .  \ncompound_tenseyes:languageSpanish  -0.77813    0.58036  -1.341 0.179994    \ncontrolyes:languageSpanish          0.75467    0.54413   1.387 0.165466    \nlanguageSpanish:reflprimingyes      0.79330    0.45855   1.730 0.083623 .  \nlanguageSpanish:caus_use           -2.57614    1.03571  -2.487 0.012871 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod4, mod3)\n\nData: data\nModels:\nmod4: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod3: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod4   36 2597.4 2833.1 -1262.7   2525.4                     \nmod3   38 2598.5 2847.3 -1261.3   2522.5 2.8745  2     0.2376\n\n\nRemove animacy:language\n\n# animacy:language\n\nmod5&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ finiteness*language+ compound_tense*language+ \n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod5)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + caus_use * poly(year, 2) + telicity *  \n    language + finiteness * language + compound_tense * language +  \n    control * language + reflpriming * language + caus_use *  \n    language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2595.8   2825.0  -1262.9   2525.8     5119 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.3056 -0.2175 -0.1155  0.2098 10.4158 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2139   0.4625  \n lemma  (Intercept) 5.1201   2.2628  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.3748     0.7392   0.507 0.612124    \ntelicitytelic                      -2.1911     0.4114  -5.326 1.01e-07 ***\nanimacyyes                          0.8713     0.2077   4.195 2.73e-05 ***\nfinitenessnonfin                    0.7091     0.2183   3.248 0.001163 ** \ncompound_tenseyes                   0.9428     0.3944   2.391 0.016825 *  \ngenreProsa                         -0.4781     0.1743  -2.743 0.006079 ** \ngenreTeatro                        -0.5118     0.2844  -1.800 0.071913 .  \ncontrolyes                         -1.1045     0.3437  -3.214 0.001309 ** \nlanguageSpanish                     0.1770     0.9835   0.180 0.857194    \nreflprimingyes                     -1.3372     0.3749  -3.567 0.000361 ***\ncaus_use                            0.3403     0.8281   0.411 0.681093    \npoly(year, 2)1                     34.7186    16.1796   2.146 0.031887 *  \npoly(year, 2)2                      1.7792    14.8574   0.120 0.904680    \ntelicitytelic:poly(year, 2)1      -23.9476    11.2428  -2.130 0.033168 *  \ntelicitytelic:poly(year, 2)2      -27.6427    10.1808  -2.715 0.006624 ** \nanimacyyes:poly(year, 2)1          -0.2325    11.4751  -0.020 0.983836    \nanimacyyes:poly(year, 2)2         -21.7332    10.9093  -1.992 0.046353 *  \ncompound_tenseyes:poly(year, 2)1  -49.6692    21.3738  -2.324 0.020134 *  \ncompound_tenseyes:poly(year, 2)2  -12.8508    20.9870  -0.612 0.540325    \ngenreProsa:poly(year, 2)1         -25.9256    12.9227  -2.006 0.044833 *  \ngenreTeatro:poly(year, 2)1          2.8839    23.3427   0.124 0.901675    \ngenreProsa:poly(year, 2)2          18.1318    11.4796   1.579 0.114226    \ngenreTeatro:poly(year, 2)2        -11.1587    22.2394  -0.502 0.615841    \ncontrolyes:poly(year, 2)1          25.3565    13.0019   1.950 0.051150 .  \ncontrolyes:poly(year, 2)2          -5.0792    12.0748  -0.421 0.674015    \ncaus_use:poly(year, 2)1           -23.3069    23.5053  -0.992 0.321412    \ncaus_use:poly(year, 2)2            28.1811    22.0188   1.280 0.200593    \ntelicitytelic:languageSpanish       1.4495     0.5023   2.886 0.003904 ** \nfinitenessnonfin:languageSpanish   -0.4902     0.2803  -1.749 0.080301 .  \ncompound_tenseyes:languageSpanish  -0.7803     0.5800  -1.345 0.178544    \ncontrolyes:languageSpanish          0.4948     0.3635   1.361 0.173460    \nlanguageSpanish:reflprimingyes      0.7956     0.4582   1.736 0.082549 .  \nlanguageSpanish:caus_use           -2.5960     1.0351  -2.508 0.012140 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod4, mod5)\n\nData: data\nModels:\nmod5: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod4: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod5   35 2595.8 2825.0 -1262.9   2525.8                     \nmod4   36 2597.4 2833.1 -1262.7   2525.4 0.4225  1     0.5157\n\n\nRemove caus_use:poly(year, 2)\n\n#caus_use:poly(year, 2)\n\nmod6&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ finiteness*language+ compound_tense*language+ \n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod6)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + finiteness *  \n    language + compound_tense * language + control * language +  \n    reflpriming * language + caus_use * language + (1 | lemma) +  \n    (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2594.4   2810.5  -1264.2   2528.4     5121 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.6787 -0.2171 -0.1152  0.2085  8.9834 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1848   0.4299  \n lemma  (Intercept) 5.1839   2.2768  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.1967     0.7281   0.270 0.787024    \ntelicitytelic                      -2.1394     0.4068  -5.260 1.44e-07 ***\nanimacyyes                          0.8684     0.2072   4.191 2.77e-05 ***\nfinitenessnonfin                    0.7148     0.2173   3.290 0.001002 ** \ncompound_tenseyes                   0.9234     0.3924   2.353 0.018616 *  \ngenreProsa                         -0.4699     0.1720  -2.731 0.006305 ** \ngenreTeatro                        -0.5216     0.2817  -1.852 0.064054 .  \ncontrolyes                         -1.1087     0.3419  -3.243 0.001185 ** \nlanguageSpanish                     0.2651     0.9820   0.270 0.787234    \nreflprimingyes                     -1.3198     0.3716  -3.552 0.000383 ***\ncaus_use                            0.5812     0.7949   0.731 0.464651    \npoly(year, 2)1                     28.2103    13.6509   2.067 0.038777 *  \npoly(year, 2)2                     12.2720    12.0529   1.018 0.308591    \ntelicitytelic:poly(year, 2)1      -29.2756    10.4086  -2.813 0.004914 ** \ntelicitytelic:poly(year, 2)2      -22.2533     9.4197  -2.362 0.018157 *  \nanimacyyes:poly(year, 2)1           0.3898    11.4405   0.034 0.972819    \nanimacyyes:poly(year, 2)2         -22.2290    10.8756  -2.044 0.040960 *  \ncompound_tenseyes:poly(year, 2)1  -44.7148    21.2026  -2.109 0.034951 *  \ncompound_tenseyes:poly(year, 2)2  -17.0961    20.8182  -0.821 0.411525    \ngenreProsa:poly(year, 2)1         -27.5889    12.7430  -2.165 0.030386 *  \ngenreTeatro:poly(year, 2)1          3.8331    23.1471   0.166 0.868474    \ngenreProsa:poly(year, 2)2          18.9237    11.3599   1.666 0.095747 .  \ngenreTeatro:poly(year, 2)2        -13.3958    22.0805  -0.607 0.544063    \ncontrolyes:poly(year, 2)1          27.2153    12.8699   2.115 0.034459 *  \ncontrolyes:poly(year, 2)2          -6.1055    12.0223  -0.508 0.611560    \ntelicitytelic:languageSpanish       1.3834     0.4966   2.786 0.005341 ** \nfinitenessnonfin:languageSpanish   -0.4947     0.2790  -1.773 0.076250 .  \ncompound_tenseyes:languageSpanish  -0.7934     0.5774  -1.374 0.169438    \ncontrolyes:languageSpanish          0.4963     0.3620   1.371 0.170362    \nlanguageSpanish:reflprimingyes      0.7952     0.4556   1.745 0.080917 .  \nlanguageSpanish:caus_use           -2.6592     1.0209  -2.605 0.009193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod6, mod5)\n\nData: data\nModels:\nmod6: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod5: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod6   33 2594.4 2810.5 -1264.2   2528.4                     \nmod5   35 2595.8 2825.0 -1262.9   2525.8 2.5807  2     0.2752\n\n\nRemove compound_tense:language\n\n#compound_tense:language\n\nmod7&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ finiteness*language+ \n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod7)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + finiteness *  \n    language + control * language + reflpriming * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2594.3   2803.9  -1265.2   2530.3     5122 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.6895 -0.2169 -0.1160  0.2087  9.1398 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1854   0.4306  \n lemma  (Intercept) 5.1974   2.2798  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2183     0.7280   0.300 0.764268    \ntelicitytelic                     -2.1272     0.4064  -5.234 1.66e-07 ***\nanimacyyes                         0.8649     0.2070   4.179 2.93e-05 ***\nfinitenessnonfin                   0.6857     0.2156   3.181 0.001470 ** \ncompound_tenseyes                  0.5360     0.2860   1.874 0.060882 .  \ngenreProsa                        -0.4599     0.1718  -2.677 0.007429 ** \ngenreTeatro                       -0.5151     0.2822  -1.826 0.067912 .  \ncontrolyes                        -1.0980     0.3414  -3.216 0.001298 ** \nlanguageSpanish                    0.2115     0.9819   0.215 0.829487    \nreflprimingyes                    -1.3198     0.3716  -3.552 0.000383 ***\ncaus_use                           0.5696     0.7929   0.718 0.472487    \npoly(year, 2)1                    27.9517    13.6427   2.049 0.040478 *  \npoly(year, 2)2                    12.0736    12.0556   1.001 0.316587    \ntelicitytelic:poly(year, 2)1     -29.0550    10.4027  -2.793 0.005222 ** \ntelicitytelic:poly(year, 2)2     -22.1117     9.4262  -2.346 0.018988 *  \nanimacyyes:poly(year, 2)1         -0.6015    11.4139  -0.053 0.957971    \nanimacyyes:poly(year, 2)2        -21.9443    10.8817  -2.017 0.043735 *  \ncompound_tenseyes:poly(year, 2)1 -46.9257    21.6675  -2.166 0.030332 *  \ncompound_tenseyes:poly(year, 2)2 -25.7056    20.3428  -1.264 0.206365    \ngenreProsa:poly(year, 2)1        -27.2029    12.7430  -2.135 0.032783 *  \ngenreTeatro:poly(year, 2)1         5.0229    23.1749   0.217 0.828411    \ngenreProsa:poly(year, 2)2         18.8820    11.3753   1.660 0.096930 .  \ngenreTeatro:poly(year, 2)2       -12.5218    22.0669  -0.567 0.570410    \ncontrolyes:poly(year, 2)1         27.8244    12.8710   2.162 0.030635 *  \ncontrolyes:poly(year, 2)2         -6.0172    12.0339  -0.500 0.617060    \ntelicitytelic:languageSpanish      1.3676     0.4969   2.752 0.005915 ** \nfinitenessnonfin:languageSpanish  -0.4503     0.2767  -1.627 0.103640    \ncontrolyes:languageSpanish         0.4967     0.3618   1.373 0.169839    \nlanguageSpanish:reflprimingyes     0.7945     0.4556   1.744 0.081165 .  \nlanguageSpanish:caus_use          -2.6388     1.0199  -2.587 0.009670 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod6, mod7)\n\nData: data\nModels:\nmod7: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod6: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod7   32 2594.3 2803.9 -1265.2   2530.3                     \nmod6   33 2594.4 2810.5 -1264.2   2528.4 1.9428  1     0.1634\n\n\nRemove control:language\n\n#control:language  \n\nmod8&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ finiteness*language+ \n              reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod8)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + finiteness *  \n    language + reflpriming * language + caus_use * language +  \n    (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2594.3   2797.3  -1266.2   2532.3     5123 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.2338 -0.2157 -0.1158  0.2070  9.0616 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1933   0.4396  \n lemma  (Intercept) 5.2888   2.2997  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2415     0.7327   0.330 0.741689    \ntelicitytelic                     -2.2645     0.3970  -5.704 1.17e-08 ***\nanimacyyes                         0.8640     0.2074   4.166 3.10e-05 ***\nfinitenessnonfin                   0.6564     0.2144   3.061 0.002205 ** \ncompound_tenseyes                  0.5214     0.2849   1.830 0.067220 .  \ngenreProsa                        -0.4658     0.1722  -2.705 0.006831 ** \ngenreTeatro                       -0.5328     0.2820  -1.890 0.058821 .  \ncontrolyes                        -0.7786     0.2535  -3.071 0.002135 ** \nlanguageSpanish                    0.2344     0.9879   0.237 0.812428    \nreflprimingyes                    -1.3302     0.3732  -3.564 0.000365 ***\ncaus_use                           0.6228     0.7957   0.783 0.433735    \npoly(year, 2)1                    27.4667    13.6730   2.009 0.044556 *  \npoly(year, 2)2                    12.0219    12.0801   0.995 0.319648    \ntelicitytelic:poly(year, 2)1     -27.9571    10.3842  -2.692 0.007096 ** \ntelicitytelic:poly(year, 2)2     -22.3772     9.4331  -2.372 0.017683 *  \nanimacyyes:poly(year, 2)1         -0.5715    11.4250  -0.050 0.960103    \nanimacyyes:poly(year, 2)2        -21.5709    10.8929  -1.980 0.047673 *  \ncompound_tenseyes:poly(year, 2)1 -46.3850    21.6474  -2.143 0.032133 *  \ncompound_tenseyes:poly(year, 2)2 -25.4520    20.3183  -1.253 0.210327    \ngenreProsa:poly(year, 2)1        -27.8006    12.7744  -2.176 0.029535 *  \ngenreTeatro:poly(year, 2)1         3.7350    23.1890   0.161 0.872039    \ngenreProsa:poly(year, 2)2         18.5382    11.3900   1.628 0.103611    \ngenreTeatro:poly(year, 2)2       -12.0271    22.0881  -0.545 0.586095    \ncontrolyes:poly(year, 2)1         26.1034    12.8150   2.037 0.041656 *  \ncontrolyes:poly(year, 2)2         -5.3265    12.0252  -0.443 0.657804    \ntelicitytelic:languageSpanish      1.5127     0.4881   3.099 0.001941 ** \nfinitenessnonfin:languageSpanish  -0.4051     0.2748  -1.474 0.140354    \nlanguageSpanish:reflprimingyes     0.8058     0.4569   1.764 0.077783 .  \nlanguageSpanish:caus_use          -2.6849     1.0219  -2.627 0.008606 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod8, mod7)\n\nData: data\nModels:\nmod8: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod7: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod8   31 2594.3 2797.3 -1266.2   2532.3                     \nmod7   32 2594.3 2803.9 -1265.2   2530.3 1.9593  1     0.1616\n\n\nRemove finiteness:language\n\n#finiteness:language \n\nmod9&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ \n              reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod9)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + reflpriming *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2594.5   2791.0  -1267.3   2534.5     5124 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.4590 -0.2169 -0.1156  0.2064  9.3486 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1889   0.4347  \n lemma  (Intercept) 5.2794   2.2977  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2871     0.7308   0.393 0.694388    \ntelicitytelic                     -2.2640     0.3951  -5.730 1.01e-08 ***\nanimacyyes                         0.8587     0.2072   4.144 3.41e-05 ***\nfinitenessnonfin                   0.4133     0.1377   3.002 0.002681 ** \ncompound_tenseyes                  0.5057     0.2841   1.780 0.075048 .  \ngenreProsa                        -0.4491     0.1712  -2.623 0.008719 ** \ngenreTeatro                       -0.5080     0.2805  -1.811 0.070142 .  \ncontrolyes                        -0.7727     0.2533  -3.051 0.002284 ** \nlanguageSpanish                    0.1385     0.9844   0.141 0.888128    \nreflprimingyes                    -1.3021     0.3686  -3.533 0.000412 ***\ncaus_use                           0.6278     0.7937   0.791 0.428996    \npoly(year, 2)1                    26.8709    13.6244   1.972 0.048579 *  \npoly(year, 2)2                    11.3566    12.0417   0.943 0.345629    \ntelicitytelic:poly(year, 2)1     -27.6962    10.3742  -2.670 0.007591 ** \ntelicitytelic:poly(year, 2)2     -22.3577     9.4265  -2.372 0.017702 *  \nanimacyyes:poly(year, 2)1         -0.9065    11.4286  -0.079 0.936782    \nanimacyyes:poly(year, 2)2        -21.3859    10.8939  -1.963 0.049635 *  \ncompound_tenseyes:poly(year, 2)1 -45.8172    21.5738  -2.124 0.033691 *  \ncompound_tenseyes:poly(year, 2)2 -24.5888    20.2270  -1.216 0.224121    \ngenreProsa:poly(year, 2)1        -27.4022    12.7236  -2.154 0.031268 *  \ngenreTeatro:poly(year, 2)1         2.7554    23.0801   0.119 0.904970    \ngenreProsa:poly(year, 2)2         18.9378    11.3598   1.667 0.095497 .  \ngenreTeatro:poly(year, 2)2       -11.3645    21.9875  -0.517 0.605253    \ncontrolyes:poly(year, 2)1         26.4304    12.8087   2.063 0.039068 *  \ncontrolyes:poly(year, 2)2         -4.8303    12.0187  -0.402 0.687760    \ntelicitytelic:languageSpanish      1.5194     0.4864   3.124 0.001785 ** \nlanguageSpanish:reflprimingyes     0.7779     0.4533   1.716 0.086170 .  \nlanguageSpanish:caus_use          -2.7015     1.0207  -2.647 0.008129 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod9, mod8)\n\nData: data\nModels:\nmod9: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod8: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod9   30 2594.5 2791.0 -1267.3   2534.5                     \nmod8   31 2594.3 2797.3 -1266.2   2532.3 2.2273  1     0.1356\n\n\nRemove language:reflpriming. This only marginally affects the fit, so we remove it for the sake of parsimony.\n\n#language:reflpriming\n\nmod10&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ \n              caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod10)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + caus_use *  \n    language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2595.7   2785.5  -1268.8   2537.7     5125 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.2456 -0.2189 -0.1147  0.2083 10.3890 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.195    0.4416  \n lemma  (Intercept) 5.212    2.2831  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2591     0.7274   0.356 0.721720    \ntelicitytelic                     -2.2369     0.3931  -5.691 1.27e-08 ***\nanimacyyes                         0.8667     0.2074   4.178 2.94e-05 ***\nfinitenessnonfin                   0.4087     0.1376   2.971 0.002967 ** \ncompound_tenseyes                  0.4930     0.2841   1.735 0.082696 .  \ngenreProsa                        -0.4533     0.1712  -2.647 0.008119 ** \ngenreTeatro                       -0.5017     0.2807  -1.787 0.073881 .  \ncontrolyes                        -0.7834     0.2534  -3.091 0.001994 ** \nlanguageSpanish                    0.1926     0.9796   0.197 0.844167    \nreflprimingyes                    -0.7965     0.2131  -3.738 0.000185 ***\ncaus_use                           0.5840     0.7903   0.739 0.459962    \npoly(year, 2)1                    28.0883    13.5922   2.067 0.038781 *  \npoly(year, 2)2                    12.0286    12.0347   0.999 0.317554    \ntelicitytelic:poly(year, 2)1     -28.9759    10.3656  -2.795 0.005184 ** \ntelicitytelic:poly(year, 2)2     -22.1901     9.4332  -2.352 0.018655 *  \nanimacyyes:poly(year, 2)1         -1.5556    11.4412  -0.136 0.891849    \nanimacyyes:poly(year, 2)2        -21.5509    10.9011  -1.977 0.048048 *  \ncompound_tenseyes:poly(year, 2)1 -44.3471    21.5677  -2.056 0.039765 *  \ncompound_tenseyes:poly(year, 2)2 -24.5765    20.2619  -1.213 0.225153    \ngenreProsa:poly(year, 2)1        -28.2452    12.6992  -2.224 0.026137 *  \ngenreTeatro:poly(year, 2)1         1.5550    23.1286   0.067 0.946395    \ngenreProsa:poly(year, 2)2         18.3940    11.3567   1.620 0.105306    \ngenreTeatro:poly(year, 2)2       -12.4002    22.0086  -0.563 0.573147    \ncontrolyes:poly(year, 2)1         27.6178    12.8266   2.153 0.031305 *  \ncontrolyes:poly(year, 2)2         -4.7322    12.0577  -0.392 0.694719    \ntelicitytelic:languageSpanish      1.4819     0.4847   3.058 0.002231 ** \nlanguageSpanish:caus_use          -2.6242     1.0181  -2.578 0.009952 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod9, mod10) #marginal=remove\n\nData: data\nModels:\nmod10: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod9: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nmod10   29 2595.7 2785.5 -1268.8   2537.7                       \nmod9    30 2594.5 2791.0 -1267.3   2534.5 3.1252  1    0.07709 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemove compound_tense:poly(year, 2). ANOVA shows it is not ok to remove, as this significantly affects the fit.\n\n#compound_tense:poly(year, 2)2\n\nmod11&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2) + animacy*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod11)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    genre * poly(year, 2) + control * poly(year, 2) + telicity *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2599.9   2776.7  -1273.0   2545.9     5127 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.1150 -0.2191 -0.1159  0.2085 10.1111 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1987   0.4458  \n lemma  (Intercept) 5.2101   2.2826  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     0.2731     0.7262   0.376 0.706892    \ntelicitytelic                  -2.2468     0.3928  -5.719 1.07e-08 ***\nanimacyyes                      0.8613     0.2071   4.158 3.21e-05 ***\nfinitenessnonfin                0.3997     0.1371   2.915 0.003552 ** \ncompound_tenseyes               0.2537     0.2823   0.899 0.368776    \ngenreProsa                     -0.4490     0.1707  -2.631 0.008513 ** \ngenreTeatro                    -0.5054     0.2800  -1.805 0.071014 .  \ncontrolyes                     -0.7598     0.2530  -3.003 0.002670 ** \nlanguageSpanish                 0.2007     0.9786   0.205 0.837503    \nreflprimingyes                 -0.7930     0.2127  -3.728 0.000193 ***\ncaus_use                        0.5923     0.7859   0.754 0.451007    \npoly(year, 2)1                 27.3278    13.5538   2.016 0.043774 *  \npoly(year, 2)2                 11.5955    12.0030   0.966 0.334020    \ntelicitytelic:poly(year, 2)1  -30.3751    10.2889  -2.952 0.003155 ** \ntelicitytelic:poly(year, 2)2  -22.7445     9.3581  -2.430 0.015080 *  \nanimacyyes:poly(year, 2)1      -1.1483    11.4135  -0.101 0.919859    \nanimacyyes:poly(year, 2)2     -21.0048    10.8748  -1.932 0.053419 .  \ngenreProsa:poly(year, 2)1     -29.1223    12.6703  -2.298 0.021536 *  \ngenreTeatro:poly(year, 2)1     -0.5833    23.0994  -0.025 0.979853    \ngenreProsa:poly(year, 2)2      17.0467    11.3134   1.507 0.131869    \ngenreTeatro:poly(year, 2)2    -15.8250    21.9098  -0.722 0.470124    \ncontrolyes:poly(year, 2)1      28.6520    12.7984   2.239 0.025175 *  \ncontrolyes:poly(year, 2)2      -4.3995    12.0422  -0.365 0.714859    \ntelicitytelic:languageSpanish   1.4747     0.4837   3.049 0.002296 ** \nlanguageSpanish:caus_use       -2.6588     1.0144  -2.621 0.008765 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod11, mod10) # not ok\n\nData: data\nModels:\nmod11: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod10: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance Chisq Df Pr(&gt;Chisq)  \nmod11   27 2599.9 2776.7 -1273.0   2545.9                      \nmod10   29 2595.7 2785.5 -1268.8   2537.7 8.265  2    0.01604 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBack to mod10, remove animacy:poly(year, 2), which only marginally affects the fit, so we remove it for the sake of parsimony.\n\n#animacy:poly(year, 2)1 \n\nmod12&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod12)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + telicity *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2595.8   2772.6  -1270.9   2541.8     5127 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-11.6137  -0.2193  -0.1148   0.2067   9.9198 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1994   0.4466  \n lemma  (Intercept) 5.2094   2.2824  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2844     0.7271   0.391 0.695712    \ntelicitytelic                     -2.2402     0.3930  -5.699 1.20e-08 ***\nanimacyyes                         0.8563     0.2069   4.140 3.48e-05 ***\nfinitenessnonfin                   0.3961     0.1372   2.887 0.003887 ** \ncompound_tenseyes                  0.4842     0.2833   1.709 0.087495 .  \ngenreProsa                        -0.4371     0.1708  -2.559 0.010490 *  \ngenreTeatro                       -0.4871     0.2813  -1.732 0.083352 .  \ncontrolyes                        -0.7756     0.2533  -3.062 0.002199 ** \nlanguageSpanish                    0.1671     0.9793   0.171 0.864520    \nreflprimingyes                    -0.7960     0.2118  -3.759 0.000171 ***\ncaus_use                           0.5272     0.7913   0.666 0.505245    \npoly(year, 2)1                    29.4819    13.4208   2.197 0.028039 *  \npoly(year, 2)2                     6.4140    11.6002   0.553 0.580316    \ntelicitytelic:poly(year, 2)1     -29.9739    10.2595  -2.922 0.003483 ** \ntelicitytelic:poly(year, 2)2     -24.9774     9.3679  -2.666 0.007670 ** \ncompound_tenseyes:poly(year, 2)1 -43.9612    21.3327  -2.061 0.039328 *  \ncompound_tenseyes:poly(year, 2)2 -24.6607    19.9895  -1.234 0.217321    \ngenreProsa:poly(year, 2)1        -30.5676    12.6635  -2.414 0.015786 *  \ngenreTeatro:poly(year, 2)1        -0.3106    23.2091  -0.013 0.989322    \ngenreProsa:poly(year, 2)2         20.2971    11.2061   1.811 0.070101 .  \ngenreTeatro:poly(year, 2)2       -11.2673    22.1161  -0.509 0.610428    \ncontrolyes:poly(year, 2)1         27.1658     9.8018   2.772 0.005580 ** \ncontrolyes:poly(year, 2)2        -19.6613     9.3335  -2.107 0.035159 *  \ntelicitytelic:languageSpanish      1.4676     0.4840   3.032 0.002429 ** \nlanguageSpanish:caus_use          -2.5584     1.0196  -2.509 0.012101 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod12, mod10) #marginal=remove\n\nData: data\nModels:\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod10: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod12   27 2595.8 2772.6 -1270.9   2541.8                     \nmod10   29 2595.7 2785.5 -1268.8   2537.7 4.1168  2     0.1277\n\n\nRemove genre:poly(year, 2). This significantly affects the fit.\n\n#genreTeatro:poly(year, 2)1 \n\nmod13&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\n#summary(mod13)\nanova(mod12, mod13) #not ok\n\nData: data\nModels:\nmod13: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)   \nmod13   23 2601.4 2752.0 -1277.7   2555.4                        \nmod12   27 2595.8 2772.6 -1270.9   2541.8 13.629  4   0.008578 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#model 12 final (not possible to remove anything else)\n\nBack to mod12, we could try to remove the simple terms that are not involved in interactions: finiteness, reflpriming, animacy: however these are all highly significant in the model summary of mod12 and removing them significantly affects the fit, as shown in the code below:\n\n# remove finiteness\n\nmod14&lt;-glmer(coding ~ telicity + animacy +  compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod14)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + compound_tense + genre + control +  \n    language + reflpriming + caus_use + poly(year, 2) + telicity *  \n    poly(year, 2) + compound_tense * poly(year, 2) + genre *  \n    poly(year, 2) + control * poly(year, 2) + telicity * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2602.4   2772.6  -1275.2   2550.4     5128 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-11.7703  -0.2226  -0.1158   0.2066   9.5674 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.184    0.4289  \n lemma  (Intercept) 5.220    2.2847  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.3630     0.7264   0.500 0.617290    \ntelicitytelic                     -2.2823     0.3915  -5.829 5.57e-09 ***\nanimacyyes                         0.8665     0.2057   4.213 2.52e-05 ***\ncompound_tenseyes                  0.4001     0.2818   1.420 0.155740    \ngenreProsa                        -0.4195     0.1695  -2.475 0.013317 *  \ngenreTeatro                       -0.4640     0.2793  -1.661 0.096641 .  \ncontrolyes                        -0.7349     0.2518  -2.918 0.003521 ** \nlanguageSpanish                    0.1661     0.9795   0.170 0.865334    \nreflprimingyes                    -0.7835     0.2100  -3.731 0.000191 ***\ncaus_use                           0.5710     0.7901   0.723 0.469831    \npoly(year, 2)1                    28.5120    13.3338   2.138 0.032490 *  \npoly(year, 2)2                     5.9569    11.5094   0.518 0.604758    \ntelicitytelic:poly(year, 2)1     -29.1353    10.2344  -2.847 0.004416 ** \ntelicitytelic:poly(year, 2)2     -25.2423     9.3258  -2.707 0.006795 ** \ncompound_tenseyes:poly(year, 2)1 -44.7673    21.2548  -2.106 0.035185 *  \ncompound_tenseyes:poly(year, 2)2 -22.0843    19.8380  -1.113 0.265609    \ngenreProsa:poly(year, 2)1        -28.5563    12.5449  -2.276 0.022826 *  \ngenreTeatro:poly(year, 2)1        -1.6448    23.0160  -0.071 0.943028    \ngenreProsa:poly(year, 2)2         19.4068    11.1233   1.745 0.081037 .  \ngenreTeatro:poly(year, 2)2       -10.1599    21.8823  -0.464 0.642435    \ncontrolyes:poly(year, 2)1         27.3656     9.7869   2.796 0.005172 ** \ncontrolyes:poly(year, 2)2        -19.8295     9.3212  -2.127 0.033391 *  \ntelicitytelic:languageSpanish      1.4977     0.4829   3.102 0.001925 ** \nlanguageSpanish:caus_use          -2.6191     1.0161  -2.578 0.009951 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod12, mod14) #not ok\n\nData: data\nModels:\nmod14: coding ~ telicity + animacy + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance Chisq Df Pr(&gt;Chisq)   \nmod14   26 2602.4 2772.6 -1275.2   2550.4                       \nmod12   27 2595.8 2772.6 -1270.9   2541.8 8.622  1   0.003321 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# remove animacy\n\nmod15&lt;-glmer(coding ~ telicity + finiteness + compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod15)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + finiteness + compound_tense + genre + control +  \n    language + reflpriming + caus_use + poly(year, 2) + telicity *  \n    poly(year, 2) + compound_tense * poly(year, 2) + genre *  \n    poly(year, 2) + control * poly(year, 2) + telicity * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2611.5   2781.7  -1279.7   2559.5     5128 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.6668 -0.2219 -0.1167  0.2019  9.8757 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1874   0.4329  \n lemma  (Intercept) 5.4416   2.3327  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.405477   0.732214   0.554 0.579738    \ntelicitytelic                     -2.228758   0.387872  -5.746 9.13e-09 ***\nfinitenessnonfin                   0.406703   0.136069   2.989 0.002799 ** \ncompound_tenseyes                  0.496729   0.281969   1.762 0.078129 .  \ngenreProsa                        -0.467873   0.169962  -2.753 0.005909 ** \ngenreTeatro                       -0.485781   0.279701  -1.737 0.082424 .  \ncontrolyes                        -0.002561   0.174363  -0.015 0.988283    \nlanguageSpanish                    0.074091   0.988589   0.075 0.940258    \nreflprimingyes                    -0.801201   0.210666  -3.803 0.000143 ***\ncaus_use                           0.542181   0.788905   0.687 0.491920    \npoly(year, 2)1                    29.190220  13.393391   2.179 0.029298 *  \npoly(year, 2)2                     6.023283  11.488203   0.524 0.600069    \ntelicitytelic:poly(year, 2)1     -31.205569  10.219843  -3.053 0.002262 ** \ntelicitytelic:poly(year, 2)2     -22.406518   9.289687  -2.412 0.015866 *  \ncompound_tenseyes:poly(year, 2)1 -42.819094  21.172315  -2.022 0.043134 *  \ncompound_tenseyes:poly(year, 2)2 -24.675497  19.914011  -1.239 0.215308    \ngenreProsa:poly(year, 2)1        -28.657149  12.580072  -2.278 0.022728 *  \ngenreTeatro:poly(year, 2)1         0.936845  23.111253   0.041 0.967666    \ngenreProsa:poly(year, 2)2         19.185881  11.070621   1.733 0.083088 .  \ngenreTeatro:poly(year, 2)2       -12.746166  21.958101  -0.580 0.561593    \ncontrolyes:poly(year, 2)1         28.008451   9.745994   2.874 0.004055 ** \ncontrolyes:poly(year, 2)2        -20.110222   9.269633  -2.169 0.030047 *  \ntelicitytelic:languageSpanish      1.499075   0.480267   3.121 0.001800 ** \nlanguageSpanish:caus_use          -2.474567   1.013529  -2.442 0.014625 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod12, mod15) #not ok\n\nData: data\nModels:\nmod15: coding ~ telicity + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmod15   26 2611.5 2781.7 -1279.8   2559.5                         \nmod12   27 2595.8 2772.6 -1270.9   2541.8 17.718  1  2.562e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# remove reflpriming\n\nmod16&lt;-glmer(coding ~ telicity + finiteness + animacy + compound_tense + genre +\n               control + language +  caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod16)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + finiteness + animacy + compound_tense + genre +  \n    control + language + caus_use + poly(year, 2) + telicity *  \n    poly(year, 2) + compound_tense * poly(year, 2) + genre *  \n    poly(year, 2) + control * poly(year, 2) + telicity * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2608.8   2779.1  -1278.4   2556.8     5128 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-11.5964  -0.2205  -0.1168   0.2060   8.7160 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2076   0.4557  \n lemma  (Intercept) 5.2234   2.2855  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2072     0.7275   0.285  0.77574    \ntelicitytelic                     -2.2234     0.3916  -5.677 1.37e-08 ***\nfinitenessnonfin                   0.3900     0.1372   2.842  0.00449 ** \nanimacyyes                         0.8620     0.2064   4.177 2.95e-05 ***\ncompound_tenseyes                  0.4806     0.2817   1.706  0.08801 .  \ngenreProsa                        -0.4178     0.1706  -2.449  0.01434 *  \ngenreTeatro                       -0.4301     0.2815  -1.528  0.12657    \ncontrolyes                        -0.7841     0.2528  -3.102  0.00192 ** \nlanguageSpanish                    0.1607     0.9807   0.164  0.86984    \ncaus_use                           0.4973     0.7926   0.627  0.53036    \npoly(year, 2)1                    29.5471    13.3535   2.213  0.02692 *  \npoly(year, 2)2                     4.6329    11.5560   0.401  0.68849    \ntelicitytelic:poly(year, 2)1     -28.0188    10.1980  -2.747  0.00601 ** \ntelicitytelic:poly(year, 2)2     -24.9587     9.3513  -2.669  0.00761 ** \ncompound_tenseyes:poly(year, 2)1 -46.0537    21.1852  -2.174  0.02972 *  \ncompound_tenseyes:poly(year, 2)2 -21.3050    19.9558  -1.068  0.28570    \ngenreProsa:poly(year, 2)1        -31.2731    12.6468  -2.473  0.01341 *  \ngenreTeatro:poly(year, 2)1        -3.2607    23.3435  -0.140  0.88891    \ngenreProsa:poly(year, 2)2         22.4521    11.2151   2.002  0.04529 *  \ngenreTeatro:poly(year, 2)2        -8.1227    22.1365  -0.367  0.71367    \ncontrolyes:poly(year, 2)1         26.2792     9.8044   2.680  0.00735 ** \ncontrolyes:poly(year, 2)2        -21.2833     9.3111  -2.286  0.02227 *  \ntelicitytelic:languageSpanish      1.4715     0.4830   3.047  0.00232 ** \nlanguageSpanish:caus_use          -2.6181     1.0214  -2.563  0.01037 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod12, mod16) #not ok\n\nData: data\nModels:\nmod16: coding ~ telicity + finiteness + animacy + compound_tense + genre + control + language + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmod16   26 2608.8 2779.1 -1278.4   2556.8                         \nmod12   27 2595.8 2772.6 -1270.9   2541.8 15.064  1  0.0001039 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe final model is mod12, which proves as the minimally adequate model\n\n\nModel report\nRefit mod12 after setting the reference level of the dependent variable data$coding to zero, and produce the reports and diagnostics of the model.\n\ndata$coding &lt;- relevel(data$coding, ref = \"zero\")  # Now \"antic\" is the level being predicted\n\nmod&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre + \n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2) + compound_tense*poly(year,2) + genre*poly(year,2) + \n               control*poly(year,2)  +\n               #language\n               telicity*language+ caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\n\nsummary(mod)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + telicity *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2595.8   2772.6  -1270.9   2541.8     5127 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-9.9198 -0.2067  0.1148  0.2193 11.6137 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1994   0.4466  \n lemma  (Intercept) 5.2094   2.2824  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       -0.2844     0.7271  -0.391 0.695712    \ntelicitytelic                      2.2402     0.3930   5.699 1.20e-08 ***\nanimacyyes                        -0.8563     0.2069  -4.140 3.48e-05 ***\nfinitenessnonfin                  -0.3961     0.1372  -2.887 0.003887 ** \ncompound_tenseyes                 -0.4842     0.2833  -1.709 0.087495 .  \ngenreProsa                         0.4371     0.1708   2.559 0.010490 *  \ngenreTeatro                        0.4871     0.2813   1.732 0.083352 .  \ncontrolyes                         0.7756     0.2533   3.062 0.002199 ** \nlanguageSpanish                   -0.1671     0.9793  -0.171 0.864520    \nreflprimingyes                     0.7960     0.2118   3.759 0.000171 ***\ncaus_use                          -0.5272     0.7913  -0.666 0.505245    \npoly(year, 2)1                   -29.4819    13.4208  -2.197 0.028039 *  \npoly(year, 2)2                    -6.4140    11.6002  -0.553 0.580316    \ntelicitytelic:poly(year, 2)1      29.9739    10.2595   2.922 0.003483 ** \ntelicitytelic:poly(year, 2)2      24.9774     9.3679   2.666 0.007670 ** \ncompound_tenseyes:poly(year, 2)1  43.9612    21.3327   2.061 0.039328 *  \ncompound_tenseyes:poly(year, 2)2  24.6607    19.9895   1.234 0.217321    \ngenreProsa:poly(year, 2)1         30.5676    12.6635   2.414 0.015786 *  \ngenreTeatro:poly(year, 2)1         0.3106    23.2091   0.013 0.989322    \ngenreProsa:poly(year, 2)2        -20.2971    11.2061  -1.811 0.070101 .  \ngenreTeatro:poly(year, 2)2        11.2673    22.1161   0.509 0.610428    \ncontrolyes:poly(year, 2)1        -27.1658     9.8018  -2.772 0.005580 ** \ncontrolyes:poly(year, 2)2         19.6613     9.3335   2.107 0.035159 *  \ntelicitytelic:languageSpanish     -1.4676     0.4840  -3.032 0.002429 ** \nlanguageSpanish:caus_use           2.5584     1.0196   2.509 0.012101 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nreport&lt;-list_report(mod, data$coding)\ndiagn&lt;-make_diagnostics(mod, data$coding)\n\n## ---------------------------------------------------------------------------------\ndata$pred_diac &lt;- predict(mod,type=\"response\")\ndata$pred_diac &lt;- ifelse(data$pred_diac&gt;0.5,\"anticaus\", \"zero\")\nprop.table(table(data$pred_diac,data$coding),1)*100\n\n          \n                zero     antic\n  anticaus  6.068189 93.931811\n  zero     89.473684 10.526316\n\npredmoddiac&lt;-as.data.frame(table(data$pred_diac, data$coding))\n\n\n## ---------------------------------------------------------------------------------\n\ndiagn %&gt;%\n  mutate_if(is.numeric, ~ round(.x, 2)) %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\nTest\nFit\n\n\n\n\nAIC\n2595.78\n\n\nBIC\n2772.56\n\n\nR$^2$ (conditional)\n0.67\n\n\nR$^2$ (marginal)\n0.12\n\n\nC\n0.97\n\n\nDxy\n0.93\n\n\nObservations\n5154.00\n\n\n\n\n## ---------------------------------------------------------------------------------\n\nreport$fixedsmall$LogOdds &lt;- log(report$fixedsmall$OR)\n\n\nreport$fixedsmall %&gt;% \n  mutate_if(is.numeric, ~ round(.x, 2)) %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\nVariable\nValue\nOR\np\nLogOdds\n\n\n\n\n\\textsc{ }\n(Intercept)\n7.500000e-01\n0.70\n-0.28\n\n\n\\textsc{Telicity}\ntelic\n9.390000e+00\n0.00***\n2.24\n\n\n\\textsc{Animacy}\nyes\n4.200000e-01\n0.00***\n-0.86\n\n\n\\textsc{Finiteness}\nnonfin\n6.700000e-01\n0.00**\n-0.40\n\n\n\\textsc{Compound Tense [Yes]}\nNA\n6.200000e-01\n0.09.\n-0.48\n\n\n\\textsc{Genre}\nProsa\n1.550000e+00\n0.01*\n0.44\n\n\n\\textsc{Genre}\nTeatro\n1.630000e+00\n0.08.\n0.49\n\n\n\\textsc{Control}\nyes\n2.170000e+00\n0.00**\n0.78\n\n\n\\textsc{Language}\nSpanish\n8.500000e-01\n0.86\n-0.17\n\n\n\\textsc{Reflpriming}\nyes\n2.220000e+00\n0.00***\n0.80\n\n\n\\textsc{Caus Use}\nN words\n5.900000e-01\n0.51\n-0.53\n\n\n\\textsc{Year}\n1st degree\n0.000000e+00\n0.03*\n-29.48\n\n\n\\textsc{Year}\n2nd degree\n0.000000e+00\n0.58\n-6.41\n\n\n\\textsc{Telicity}\ntelic × year [1st degree]\n1.041106e+13\n0.00**\n29.97\n\n\n\\textsc{Telicity}\ntelic × year [2nd degree]\n7.039504e+10\n0.01**\n24.98\n\n\n\\textsc{Compound Tense [Yes] × Year [1st Degree]}\nNA\n1.236238e+19\n0.04*\n43.96\n\n\n\\textsc{Compound Tense [Yes] × Year [2nd Degree]}\nNA\n5.128566e+10\n0.22\n24.66\n\n\n\\textsc{Genre}\nProsa × year [1st degree]\n1.885190e+13\n0.02*\n30.57\n\n\n\\textsc{Genre}\nTeatro × year [1st degree]\n1.360000e+00\n0.99\n0.31\n\n\n\\textsc{Genre}\nProsa × year [2nd degree]\n0.000000e+00\n0.07.\n-20.30\n\n\n\\textsc{Genre}\nTeatro × year [2nd degree]\n7.822168e+04\n0.61\n11.27\n\n\n\\textsc{Control}\nyes × year [1st degree]\n0.000000e+00\n0.01**\n-27.17\n\n\n\\textsc{Control}\nyes × year [2nd degree]\n3.457896e+08\n0.04*\n19.66\n\n\n\\textsc{Telicity}\ntelic × language [Spanish]\n2.300000e-01\n0.00**\n-1.47\n\n\n\\textsc{Language}\nSpanish × caus use\n1.292000e+01\n0.01*\n2.56\n\n\n\n\n\n\n\nModel Visualisation\n\nRandom effects\nFigure 4:\n\nranef_data &lt;- lme4::ranef(mod)[[\"lemma\"]]\n\nnames &lt;- rownames(ranef_data)\nrownames(ranef_data) &lt;- NULL\nranef_data &lt;- cbind(names,ranef_data)\n\nranefdata2 &lt;-\n  left_join(ranef_data,\n            data %&gt;% dplyr::select(lemma, language)%&gt;% rename(names=lemma) %&gt;% unique,\n            by = \"names\")\n\nranefdata2&lt;- ranefdata2 %&gt;% \n  rename(Intercept = `(Intercept)`, Variable= names) %&gt;% \n  mutate(color = ifelse(language == \"italian\", \"red\", \"#f1947a\"))\n\ncolors &lt;- ranefdata2$color[order(ranefdata2$Intercept)]\n\nranefdata2 %&gt;% \n  ggplot(aes(y = Intercept, x = reorder(Variable, Intercept), fill=language, pattern=Intercept&gt;0)) + \n  geom_bar_pattern(stat=\"identity\",\n                   position = position_dodge(0.9), \n                   pattern_fill = \"black\",\n                   pattern_angle = 45,\n                   pattern_density = 0.1,\n                   pattern_spacing = 0.025,\n                   pattern_key_scale_factor = 0.6,\n                   alpha=0.8,\n                   colour=\"#e9ecef\") +\n  scale_fill_manual(name = \"Language\", values = setNames(c(\"red\", \"#f1947a\"),c(\"italian\", \"spanish\")), labels = c( \"italian\", \"spanish\"))+\n  scale_pattern_manual(values = c(\"none\", \"stripe\"), labels=c(\"zero\", \"anticausative\"))+\n  labs(y = \"Log-odds\", x=\"LEMMA\", pattern= \"Coding pattern\") +\n  coord_flip() +\n  theme_minimal(base_size = 9)+\n  guides(pattern = guide_legend(override.aes = list(fill = \"grey\")),\n         fill = guide_legend(override.aes = list(pattern = \"none\")))+\n  theme(axis.text.y = element_text(colour= colors))\n\n\n\n\n\n\n\n\n\n\nSimple effects\nFigure 5:\n\neffects_list2 &lt;- purrr::map(c( \"finiteness\",\"reflpriming\", \"animacy\"),\n                           ~simpleff(mod, .x) + \n    theme_minimal(base_size = 9, base_family = \"cambria\")+\n    labs(x=\"\", y=\"\")+\n    theme(text = element_text(colour = \"black\"),\n          axis.text = element_text(colour = \"black\")\n    )\n)\n\ncowplot::plot_grid(plotlist = effects_list2,  align = \"v\")\n\n\n\n\n\n\n\n\nFigure 6:\n\nyear_eff&lt;-makeff(mod, \"year\")\nyearplot&lt;-ggplot(year_eff, aes(x=x, y=predicted, group=1)) +\n  geom_line(size=0.7, color=safe[1])+\n  labs(x= \"Real time\", y=\"\")+\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6), limits = c(0,1), labels = scales::percent)+\n  theme_minimal(base_size = 9)\n\nyearplot&lt;-yearplot+ggtitle(\"YEAR\")\nyearplot +  theme_minimal(base_size = 15) + geom_line(size=1.2, color=safe[1])\n\n\n\n\n\n\n\n\n\n\nInteractions\nFigure 7:\n\ndiacsmooth(mod, var=c(\"year\", \"compound_tense\"))\n\n\n\n\n\n\n\n\nFigure 8:\n\ndiacsmooth(mod, var=c(\"year\", \"control\"))\n\n\n\n\n\n\n\n\nFigure 9:\n\ndiacsmooth(mod, var=c(\"year\", \"telicity\"))\n\n\n\n\n\n\n\n\nFigure 10\n\ndiacsmooth(mod, var=c(\"year\", \"genre\"))\n\n\n\n\n\n\n\n\nFigure 11:\n\ninterplot(mod, var=c(\"telicity\", \"language\"))\n\n\n\n\n\n\n\n\nFigure 12:\n\ndf&lt;-table(data$azion, data$coding, data$language) %&gt;% as.data.frame()\ncolnames(df)&lt;-c(\"var\", \"altern\", \"language\", \"freq\")\ndf$var&lt;-df$var %&gt;%  fct_relevel(c(\"accomplishment\", \"achievement\", \"degree\", \"activity\"))\n \n ggplot(df, aes(y=freq, x=var, fill=altern)) +\n     geom_bar(position=\"fill\", stat=\"identity\", alpha=0.8)+\n     scale_y_continuous(labels = scales::percent)+\n     theme(legend.title = element_blank())+\n     labs (x=\"\", y= \"\")+ coord_flip()+\n     facet_wrap(vars(language), ncol = 5)+\n     scale_fill_manual(values=safe[c(2,1)])\n\n\n\n\n\n\n\n\nFigure 13:\n\ndiacsmooth(mod, var=c(\"caus_use\", \"language\"))+xlab(\"Causal %\")\n\n\n\n\n\n\n\n\nFigure 14:\n\ncaus_eff&lt;-makeff(mod, \"caus_use\")\n\ncausplot&lt;-ggplot(caus_eff, aes(x=x, y=predicted, group=1)) +\n  geom_line(size=1.2, color=safe[1])+\n  labs(title= \"CAUSALNESS DEGREE\", x=\"Causalness degree\", y=\"\")+\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6), limits = c(0,1), labels = scales::percent)+\n  theme_minimal(base_size = 9)\n\ncausplot +  theme_minimal(base_size = 15) + geom_line(size=1.2, color=safe[1])"
  },
  {
    "objectID": "LICENSE-data.html",
    "href": "LICENSE-data.html",
    "title": "",
    "section": "",
    "text": "Creative Commons Attribution 4.0 International (CC BY 4.0)\nThis work is licensed under the Creative Commons Attribution 4.0 International License.\nYou are free to:\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material for any purpose, even commercially\n\nUnder the following terms:\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made.\n\nNo additional restrictions — You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.\nFull license text: https://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "",
    "text": "Replication materials for: Inglese, Mazzola, Goria, Ferrarotti & Cornillie, “Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian” in Inglese, Mazzola & Goria Diachronic and Typological Perspectives on Anticausativization (Accepted December 2025). Full citation information will be released after publication."
  },
  {
    "objectID": "index.html#load-and-prepare-data",
    "href": "index.html#load-and-prepare-data",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "Load and Prepare Data",
    "text": "Load and Prepare Data\n\n# List of packages used in this document\npkgs &lt;- c(\"tidyverse\", \"lme4\", \"kableExtra\", \"readxl\", \n          \"Boruta\", \"ranger\", \"vip\", \"dplyr\", \n          \"purrr\", \"stringr\", \"ggpattern\")\n\n# Install any missing packages\nnew_pkgs &lt;- pkgs[!(pkgs %in% installed.packages()[, \"Package\"])]\nif(length(new_pkgs)) install.packages(new_pkgs)\n\n# Load all packages\nlapply(pkgs, library, character.only = TRUE)\n\n\ndata_all &lt;- read_excel(\"romall_new_VNC_pub.xlsx\")\ndata &lt;- read_excel(\"Inglese_et_al_noncaus_july25_newVNC_pub.xlsx\")\n\nData preparation, summary and NAs check:\n\ndata$period &lt;- data$vnc_period_apr25\ndata$caus_use &lt;- data$caus_use_jul25\n\nsummarydata &lt;- fct_count(data$coding)\nlanguagesummary &lt;- fct_count(data$language)\n\nvars_to_check &lt;- c(\"coding\", \"telicity\", \"animacy\", \"finiteness\", \"genre\", \"control\", \"language\", \n                   \"reflpriming\", \"caus_use\", \"year\")\n\nany_na_rows &lt;- data[!complete.cases(data[, vars_to_check]), ]\nn_na &lt;- nrow(any_na_rows)\ncat(\"Number of rows with NA in model variables:\", n_na, \"\\n\")\n\nNumber of rows with NA in model variables: 0 \n\nna_counts &lt;- colSums(is.na(data[, vars_to_check]))\nna_counts[na_counts &gt; 0]\n\nnamed numeric(0)\n\ndata &lt;- data %&gt;% mutate(across(\n  c(coding, telicity, animacy, finiteness, compound_tense, genre, time, \n    control, aspect, language, mood, subjcoding, reflpriming), \n  ~factor(.x)))\n\nlevels(data$language) &lt;- str_to_sentence(levels(data$language))"
  },
  {
    "objectID": "index.html#rationale",
    "href": "index.html#rationale",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "Rationale",
    "text": "Rationale\n\n\n\n\n\n\nObject of the study\n\n\n\nA comparative diachronic corpus study of Italian and Spanish focuses on the alternation between anticausativization (reflexive marking) and lability as noncausal marking strategies.\n\n\n\n\n\n\n\n\nResearch Questions\n\n\n\n\nWhat are the factors that influence the choice between the anticausative and the labile strategies in Italian and Spanish?\n\nDo these factors change over time?\n\nDo the strength and relevance of these factors change when Italian and Spanish are compared?\n\n\n\n\n\n\n\n\n\nData\n\n\n\n\nItalian: MIDIA corpus (D’Achille & Grossmann 2017), ~7.8 million words, balanced across tokens and genres (13th–mid-20th centuries).\n\nSpanish: Corpus del Diccionario Histórico del Español (CDH, Real Academia Española 2013), Peninsular Spanish only.\nWe extracted a sample of occurrences for the Italian and Spanish equivalents of the 20 verb meaning pairs listed in Haspelmath et al. (2014). See paper for the complete procedure.\n\n\n\nThe complete dataset includes causal and noncausal uses of the verbs extracted, contained in the dataset data_all. For this study we only include noncausal observations and only verbs with variability (data). We therefore removed the verbs with categorical selection of wither anticausative or labile marking. Figure 2 and 3 show the lemmas per language and the proportion of anticausative vs. labile marking, before filtering to only include the vairable contexts.\nFigure 1:\n\n# ---- Italian data ----\nitalian_data &lt;- data_all %&gt;%\n  filter(semantics == \"noncaus\", language == \"italian\") %&gt;%\n  group_by(lemma, coding) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  complete(lemma, coding = c(\"antic\", \"zero\"), fill = list(n = 0)) %&gt;%\n  group_by(lemma) %&gt;%\n  mutate(perc = n / sum(n) * 100) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    coding_label = recode(coding, \"antic\" = \"Anticausative\", \"zero\" = \"Labile\"),\n    coding_label = factor(coding_label, levels = c(\"Anticausative\", \"Labile\"))\n  )\n\n# Order lemmas by Anticausative %\nitalian_order &lt;- italian_data %&gt;%\n  filter(coding_label == \"Anticausative\") %&gt;%\n  arrange(desc(perc)) %&gt;%\n  pull(lemma)\n\nitalian_data &lt;- italian_data %&gt;%\n  mutate(lemma = factor(lemma, levels = italian_order))\n\n# ---- Plot Italian ----\nplot_italian &lt;- ggplot(italian_data, aes(x = lemma, y = perc, fill = coding_label)) +\n  geom_col(position = \"stack\") +\n  scale_fill_manual(values = c(\"Anticausative\" = \"grey30\", \"Labile\" = \"grey80\")) +\n  labs(x = \"Verbs\", y = \"Percentage\", fill = \"Patterns\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n# ---- Print plots ----\nplot_italian\n\n\n\n\n\n\n\n\nFigure 2:\n\n# ---- Spanish data ----\nspanish_data &lt;- data_all %&gt;%\n  filter(semantics == \"noncaus\", language == \"spanish\") %&gt;%\n  group_by(lemma, coding) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  complete(lemma, coding = c(\"antic\", \"zero\"), fill = list(n = 0)) %&gt;%\n  group_by(lemma) %&gt;%\n  mutate(perc = n / sum(n) * 100) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    coding_label = recode(coding, \"antic\" = \"Anticausative\", \"zero\" = \"Labile\"),\n    coding_label = factor(coding_label, levels = c(\"Anticausative\", \"Labile\"))\n  )\n\n\n# Order lemmas by Anticausative %\nspanish_order &lt;- spanish_data %&gt;%\n  filter(coding_label == \"Anticausative\") %&gt;%\n  arrange(desc(perc)) %&gt;%\n  pull(lemma)\n\nspanish_data &lt;- spanish_data %&gt;%\n  mutate(lemma = factor(lemma, levels = spanish_order))\n\n# ---- Plot Spanish ----\nplot_spanish &lt;- ggplot(spanish_data, aes(x = lemma, y = perc, fill = coding_label)) +\n  geom_col(position = \"stack\") +\n  scale_fill_manual(values = c(\"Anticausative\" = \"grey30\", \"Labile\" = \"grey80\")) +\n  labs(x = \"Verbs\", y = \"Percentage\", fill = \"Patterns\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\nplot_spanish\n\n\n\n\n\n\n\n\nThe filtered dataset used for this study is contained in data and includes 5154 noncausal observations. The variables used in the following statistical analysis were manually annotated and are distributed as reported in the table below (Table 1 in the paper):\n\nvars_to_check &lt;- c(\n  \"coding\", \"telicity\", \"animacy\", \"finiteness\", \"genre\",\n  \"control\", \"language\", \"reflpriming\"\n)\n\n# ---- Factor variables ----\ntable_factors &lt;- map_dfr(vars_to_check, function(v) {\n\n  data %&gt;%\n    count(.data[[v]]) %&gt;%\n    mutate(\n      Variables = str_to_sentence(v),\n      Values = str_to_sentence(as.character(.data[[v]])),\n      `N. occurrences` = n\n    ) %&gt;%\n    select(Variables, Values, `N. occurrences`)\n})\n\n# ---- Numeric variables (manual add) ----\ntable_numeric &lt;- tibble(\n  Variables = str_to_sentence(c(\"caus_use\", \"year\")),\n  Values = c(\n    paste0(min(data$caus_use, na.rm = TRUE), \" – \",\n           max(data$caus_use, na.rm = TRUE)),\n    paste0(min(data$year, na.rm = TRUE), \" – \",\n           max(data$year, na.rm = TRUE))\n  ),\n  `N. occurrences` = NA_integer_\n)\n\n# ---- Combine & print ----\nbind_rows(table_factors, table_numeric) %&gt;%\n  kbl(\n    booktabs = TRUE,\n    align = \"l\",\n    caption = \"Distribution of variables\"\n  ) %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    bootstrap_options = c(\"striped\", \"hover\")\n  ) %&gt;%\n  collapse_rows(\n    columns = 1,\n    valign = \"top\"\n  )\n\n\nDistribution of variables\n\n\nVariables\nValues\nN. occurrences\n\n\n\n\nCoding\nAntic\n3209\n\n\nZero\n1945\n\n\nTelicity\nAtelic\n1612\n\n\nTelic\n3542\n\n\nAnimacy\nNo\n2783\n\n\nYes\n2371\n\n\nFiniteness\nFin\n4055\n\n\nNonfin\n1099\n\n\nGenre\nPoesia\n997\n\n\nProsa\n3782\n\n\nTeatro\n375\n\n\nControl\nNo\n3367\n\n\nYes\n1787\n\n\nLanguage\nItalian\n2468\n\n\nSpanish\n2686\n\n\nReflpriming\nNo\n4648\n\n\nYes\n506\n\n\nCaus_use\n0 – 0.939393939393939\nNA\n\n\nYear\n1140 – 2001\nNA"
  },
  {
    "objectID": "index.html#random-forest-for-variable-importance",
    "href": "index.html#random-forest-for-variable-importance",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "Random Forest for Variable Importance",
    "text": "Random Forest for Variable Importance\nAs a first step in exploring our data, we fitted a random forest (RF) model to inspect the relative importance of the predictors. RF are a type of decision tree ensemble model that can be used to detect patterns in the data and assess which variables are most strongly associated with the outcome (Levshina 2020), and help us making decisions about the regression model.\n\nset.seed(123)\nrf1 &lt;- ranger(coding ~ year + caus_use_jul25 + \n                telicity + animacy + finiteness + compound_tense + \n                genre + time + control + aspect + language + mood +  \n                subjcoding + reflpriming,\n              data = data, importance = \"impurity_corrected\")\n\nFigure 3:\n\nrflabels&lt;- c( \"Compound tense\",\"Tense\", \"Mood\", \"Reflexive Priming\", \"Aspect\",  \"Subject Coding\", \"Finiteness\", \"Language\", \"Genre\", \"Animacy\", \"Year\", \"Control\", \"Causalness degree\", \"Telicity\")\n\nrf_plot&lt;-vip(rf1, num_features = 15) + scale_x_discrete(labels = rflabels)\n\nrf_plot\n\n\n\n\n\n\n\n\nRandom Forest - Variables imporance:\n\nvip:::vi(rf1) %&gt;% \n  mutate_if(is.numeric, ~ round(.x, 2)) %&gt;%\n  kbl() %&gt;% \n  kable_styling()\n\n\n\n\nVariable\nImportance\n\n\n\n\ntelicity\n650.30\n\n\ncaus_use_jul25\n321.60\n\n\ncontrol\n86.22\n\n\nyear\n47.00\n\n\nanimacy\n28.38\n\n\ngenre\n21.84\n\n\nlanguage\n16.64\n\n\naspect\n8.36\n\n\nfiniteness\n7.91\n\n\nsubjcoding\n7.13\n\n\nreflpriming\n6.75\n\n\ntime\n3.11\n\n\nmood\n2.27\n\n\ncompound_tense\n0.70\n\n\n\n\n\nRandom Forest - Model diagnostics\n\nrf2 &lt;- ranger(coding ~ year +  caus_use_jul25 + \n                telicity + animacy + finiteness + compound_tense + \n                genre + time + control + aspect + language + mood +  \n                subjcoding + reflpriming,\n              data = data)\n\nrf2_pred_df &lt;- bind_cols(data, .pred = predict(rf2, data)$predictions)\n\ndiagnostics &lt;- Hmisc::somers2(as.numeric(rf2_pred_df$.pred) - 1, \n                               as.numeric(rf2_pred_df$coding) - 1) %&gt;% \n  enframe() %&gt;% \n  mutate(value = round(value, 2))\n\ndiagnostics %&gt;% kbl() %&gt;% kable_styling()\n\n\n\n\nname\nvalue\n\n\n\n\nC\n0.93\n\n\nDxy\n0.86\n\n\nn\n5154.00\n\n\nMissing\n0.00"
  },
  {
    "objectID": "index.html#mixed-effect-logistic-regression",
    "href": "index.html#mixed-effect-logistic-regression",
    "title": "Anticausativization and lability in Romance: a historical corpus study on Spanish and Italian",
    "section": "Mixed-effect logistic regression",
    "text": "Mixed-effect logistic regression\nA logistic regression model with mixed effects predicts the outcome of a binary variable (in our case, SE vs. lability) given multiple explanatory factors (and their interactions), and also includes control variables, called random effects. These models are useful when the data have a hierarchical or grouped structure, as they allow us to account for variability due to such groupings, —here, verbs (LEMMA) and authors (AUTHOR).\nThe models were calculated using the lme4::glmer function (Bates et al. 2015), fitting a maximal interaction model, i.e., including all interactions between predictors and the variables YEAR or LANGUAGE.\n\nPolynomial\nWe do not assume a linear relation between year and the other predictors, i.e. we appreciate that the effects could fluctuate over time, losing predictive power or direction of the effects. This is why we included YEAR as a polynomial.\nWe fitted a linear, quadratic and a cubic model (only with simple effects). We compared the AIC, BIC and used ANOVA to compare pairs of nested models. All these tests showed that a quadratic polynomial is the best fit for the diachronic development of the dependent variable (SE vs. lability): it improves the fit compared to the linear model and it does not need a cubic term; AIC and BIC for the quadratic models are the lowest.\n\n# Linear\nmod_lin &lt;- glmer(coding ~ telicity + animacy + finiteness + compound_tense +genre +\n                control + language + reflpriming + caus_use + poly(year,1) +\n                (1|lemma) + (1|author),\n                family=\"binomial\", data=data, nAGQ=0)\n# Quadratic\n\n\nmod_quad &lt;- glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n                control + language + reflpriming + caus_use + poly(year,2) +\n                (1|lemma) + (1|author),\n                family=\"binomial\", data=data, nAGQ=0)\n\n# Cubic\n\nmod_cub &lt;- glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n                control + language + reflpriming + caus_use + poly(year,3) +\n                (1|lemma) + (1|author),\n                family=\"binomial\", data=data, nAGQ=0)\n\nAIC(mod_lin, mod_cub, mod_quad)\n\n         df      AIC\nmod_lin  14 2643.264\nmod_cub  16 2642.871\nmod_quad 15 2641.216\n\nBIC(mod_lin, mod_cub, mod_quad)\n\n         df      BIC\nmod_lin  14 2734.929\nmod_cub  16 2747.631\nmod_quad 15 2739.429\n\nanova(mod_lin, mod_quad)\n\nData: data\nModels:\nmod_lin: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 1) + (1 | lemma) + (1 | author)\nmod_quad: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + (1 | lemma) + (1 | author)\n         npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nmod_lin    14 2643.3 2734.9 -1307.6   2615.3                       \nmod_quad   15 2641.2 2739.4 -1305.6   2611.2 4.0478  1    0.04423 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod_cub, mod_quad)\n\nData: data\nModels:\nmod_quad: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + (1 | lemma) + (1 | author)\nmod_cub: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 3) + (1 | lemma) + (1 | author)\n         npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod_quad   15 2641.2 2739.4 -1305.6   2611.2                     \nmod_cub    16 2642.9 2747.6 -1305.4   2610.9 0.3454  1     0.5567\n\n\n\n\nModel selection\nThe most parsimonious interaction model was found by performing a step-wise backward selection procedure, which consists in removing non-significant interactions one by one, starting from the one with the highest p-value.\nFirst we fit the maximal model with all variables and interactions:\n\nmod_all&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n                      control + language + reflpriming + caus_use + poly(year,2) +\n                #year\n                  telicity*poly(year,2) + animacy*poly(year,2) + finiteness*poly(year,2) + compound_tense*poly(year,2) + \n                 genre*poly(year,2) + control*poly(year,2) + language*poly(year,2) + \n                 reflpriming*poly(year,2) + caus_use*poly(year,2) +\n                #language\n                  telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ genre*language+\n                  control*language+ reflpriming*language+ caus_use*language+\n                #random\n                (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\n\nsummary(mod_all)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    finiteness * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + language *  \n    poly(year, 2) + reflpriming * poly(year, 2) + caus_use *  \n    poly(year, 2) + telicity * language + animacy * language +  \n    finiteness * language + compound_tense * language + genre *  \n    language + control * language + reflpriming * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2609.9   2898.0  -1261.0   2521.9     5110 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.7873 -0.2150 -0.1132  0.2094 11.1910 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2164   0.4652  \n lemma  (Intercept) 5.0986   2.2580  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.40499    0.74866   0.541 0.588540    \ntelicitytelic                      -2.19744    0.41309  -5.319 1.04e-07 ***\nanimacyyes                          1.05674    0.37433   2.823 0.004758 ** \nfinitenessnonfin                    0.72955    0.22499   3.243 0.001185 ** \ncompound_tenseyes                   0.95798    0.39661   2.415 0.015718 *  \ngenreProsa                         -0.59627    0.22444  -2.657 0.007891 ** \ngenreTeatro                        -0.35637    0.32793  -1.087 0.277152    \ncontrolyes                         -1.27666    0.44580  -2.864 0.004186 ** \nlanguageSpanish                     0.08508    1.02514   0.083 0.933856    \nreflprimingyes                     -1.31152    0.37642  -3.484 0.000494 ***\ncaus_use                            0.32063    0.83799   0.383 0.701999    \npoly(year, 2)1                     33.39782   16.53895   2.019 0.043452 *  \npoly(year, 2)2                      0.44726   16.08470   0.028 0.977816    \ntelicitytelic:poly(year, 2)1      -24.28686   11.33634  -2.142 0.032162 *  \ntelicitytelic:poly(year, 2)2      -26.95656   10.30288  -2.616 0.008886 ** \nanimacyyes:poly(year, 2)1          -1.28888   11.77304  -0.109 0.912824    \nanimacyyes:poly(year, 2)2         -20.63096   11.24370  -1.835 0.066522 .  \nfinitenessnonfin:poly(year, 2)1    -3.49210   10.77803  -0.324 0.745937    \nfinitenessnonfin:poly(year, 2)2     0.99589   10.21325   0.098 0.922322    \ncompound_tenseyes:poly(year, 2)1  -50.32779   21.42149  -2.349 0.018803 *  \ncompound_tenseyes:poly(year, 2)2  -15.57524   21.04247  -0.740 0.459190    \ngenreProsa:poly(year, 2)1         -27.31368   14.04492  -1.945 0.051807 .  \ngenreTeatro:poly(year, 2)1          9.22838   23.68108   0.390 0.696763    \ngenreProsa:poly(year, 2)2          18.22159   12.75048   1.429 0.152978    \ngenreTeatro:poly(year, 2)2         -4.77803   22.99945  -0.208 0.835428    \ncontrolyes:poly(year, 2)1          26.87501   13.17733   2.039 0.041401 *  \ncontrolyes:poly(year, 2)2          -5.80183   12.27343  -0.473 0.636417    \nlanguageSpanish:poly(year, 2)1      4.64850   11.78480   0.394 0.693250    \nlanguageSpanish:poly(year, 2)2     -0.18386   11.76494  -0.016 0.987531    \nreflprimingyes:poly(year, 2)1       8.44986   15.24258   0.554 0.579333    \nreflprimingyes:poly(year, 2)2       3.33422   14.03653   0.238 0.812239    \ncaus_use:poly(year, 2)1           -23.56069   23.89754  -0.986 0.324180    \ncaus_use:poly(year, 2)2            28.51399   22.17092   1.286 0.198409    \ntelicitytelic:languageSpanish       1.42471    0.50389   2.827 0.004692 ** \nanimacyyes:languageSpanish         -0.25338    0.45267  -0.560 0.575650    \nfinitenessnonfin:languageSpanish   -0.51375    0.28422  -1.808 0.070666 .  \ncompound_tenseyes:languageSpanish  -0.76066    0.58092  -1.309 0.190392    \ngenreProsa:languageSpanish          0.21263    0.34075   0.624 0.532630    \ngenreTeatro:languageSpanish        -0.70863    0.61600  -1.150 0.249996    \ncontrolyes:languageSpanish          0.73903    0.54686   1.351 0.176569    \nlanguageSpanish:reflprimingyes      0.77442    0.46226   1.675 0.093876 .  \nlanguageSpanish:caus_use           -2.53425    1.06053  -2.390 0.016866 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBy inspecting the model summary we procede with the elimination of the first least significant intercation, language*poly(year,2).\n\n# remove language:year\n\nmod1&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + finiteness*poly(year,2) + compound_tense*poly(year,2) + genre*poly(year,2) +\n              control*poly(year,2) + reflpriming*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ genre*language+\n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod1)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    finiteness * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + reflpriming *  \n    poly(year, 2) + caus_use * poly(year, 2) + telicity * language +  \n    animacy * language + finiteness * language + compound_tense *  \n    language + genre * language + control * language + reflpriming *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2606.1   2881.1  -1261.0   2522.1     5112 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.3982 -0.2150 -0.1139  0.2102 11.1985 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2177   0.4666  \n lemma  (Intercept) 5.0961   2.2575  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.39230    0.74661   0.525 0.599277    \ntelicitytelic                      -2.19729    0.41354  -5.313 1.08e-07 ***\nanimacyyes                          1.05367    0.37369   2.820 0.004808 ** \nfinitenessnonfin                    0.72751    0.22425   3.244 0.001178 ** \ncompound_tenseyes                   0.95198    0.39615   2.403 0.016257 *  \ngenreProsa                         -0.59575    0.22425  -2.657 0.007892 ** \ngenreTeatro                        -0.35021    0.32739  -1.070 0.284762    \ncontrolyes                         -1.28913    0.44443  -2.901 0.003724 ** \nlanguageSpanish                     0.07796    1.02413   0.076 0.939318    \nreflprimingyes                     -1.31249    0.37632  -3.488 0.000487 ***\ncaus_use                            0.34969    0.83135   0.421 0.674023    \npoly(year, 2)1                     33.93167   16.48071   2.059 0.039507 *  \npoly(year, 2)2                     -0.21185   15.44538  -0.014 0.989056    \ntelicitytelic:poly(year, 2)1      -24.40141   11.33997  -2.152 0.031413 *  \ntelicitytelic:poly(year, 2)2      -26.75610   10.28271  -2.602 0.009267 ** \nanimacyyes:poly(year, 2)1          -0.75738   11.70432  -0.065 0.948406    \nanimacyyes:poly(year, 2)2         -20.71886   11.15169  -1.858 0.063181 .  \nfinitenessnonfin:poly(year, 2)1    -3.72487   10.77288  -0.346 0.729520    \nfinitenessnonfin:poly(year, 2)2     1.14890   10.21358   0.112 0.910437    \ncompound_tenseyes:poly(year, 2)1  -50.65284   21.40967  -2.366 0.017987 *  \ncompound_tenseyes:poly(year, 2)2  -15.84424   21.05255  -0.753 0.451688    \ngenreProsa:poly(year, 2)1         -25.49010   13.06657  -1.951 0.051082 .  \ngenreTeatro:poly(year, 2)1          9.31678   23.45156   0.397 0.691163    \ngenreProsa:poly(year, 2)2          18.91926   11.80235   1.603 0.108933    \ngenreTeatro:poly(year, 2)2         -3.43107   22.74751  -0.151 0.880108    \ncontrolyes:poly(year, 2)1          26.81281   13.18044   2.034 0.041923 *  \ncontrolyes:poly(year, 2)2          -6.01864   12.26139  -0.491 0.623524    \nreflprimingyes:poly(year, 2)1       8.74942   15.21229   0.575 0.565187    \nreflprimingyes:poly(year, 2)2       3.16663   14.01661   0.226 0.821264    \ncaus_use:poly(year, 2)1           -22.37107   23.66925  -0.945 0.344580    \ncaus_use:poly(year, 2)2            28.64179   22.13466   1.294 0.195673    \ntelicitytelic:languageSpanish       1.42524    0.50422   2.827 0.004705 ** \nanimacyyes:languageSpanish         -0.24983    0.45207  -0.553 0.580510    \nfinitenessnonfin:languageSpanish   -0.50645    0.28289  -1.790 0.073403 .  \ncompound_tenseyes:languageSpanish  -0.74728    0.57946  -1.290 0.197188    \ngenreProsa:languageSpanish          0.23142    0.33402   0.693 0.488414    \ngenreTeatro:languageSpanish        -0.67788    0.60970  -1.112 0.266211    \ncontrolyes:languageSpanish          0.74910    0.54593   1.372 0.170013    \nlanguageSpanish:reflprimingyes      0.77459    0.46234   1.675 0.093860 .  \nlanguageSpanish:caus_use           -2.55745    1.04522  -2.447 0.014413 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod_all, mod1)\n\nData: data\nModels:\nmod1: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + reflpriming * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming *      language + caus_use * language + (1 | lemma) + (1 | author)\nmod_all: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + language * poly(year, 2) + reflpriming * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control *      language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n        npar    AIC    BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod1      42 2606.1 2881.1  -1261   2522.1                     \nmod_all   44 2609.9 2898.0  -1261   2521.9 0.1625  2      0.922\n\n\nThe ANOVA comparison shows that mod1 is better than mod_all, as removing the interaction does not significantly affect the model fit (the p-value is below significance level). We proceed by removing the least significant interactions terms one-by-one and compare with ANOVA, until we reach the final model, model12.\nRemove reflpriming:poly(year, 2)\n\n# remove reflprimingyes:poly(year, 2)\n\nmod2&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + finiteness*poly(year,2) + compound_tense*poly(year,2) + genre*poly(year,2) +\n              control*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ genre*language+\n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod2)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    finiteness * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use *  \n    poly(year, 2) + telicity * language + animacy * language +  \n    finiteness * language + compound_tense * language + genre *  \n    language + control * language + reflpriming * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2602.4   2864.3  -1261.2   2522.4     5114 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.3099 -0.2148 -0.1138  0.2103 10.5369 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2174   0.4663  \n lemma  (Intercept) 5.1010   2.2585  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.39195    0.74660   0.525 0.599599    \ntelicitytelic                      -2.19692    0.41351  -5.313 1.08e-07 ***\nanimacyyes                          1.05213    0.37349   2.817 0.004848 ** \nfinitenessnonfin                    0.73052    0.22406   3.260 0.001113 ** \ncompound_tenseyes                   0.94581    0.39615   2.388 0.016963 *  \ngenreProsa                         -0.59102    0.22395  -2.639 0.008312 ** \ngenreTeatro                        -0.34893    0.32808  -1.064 0.287538    \ncontrolyes                         -1.28918    0.44436  -2.901 0.003717 ** \nlanguageSpanish                     0.09488    1.02280   0.093 0.926091    \nreflprimingyes                     -1.32751    0.37597  -3.531 0.000414 ***\ncaus_use                            0.34579    0.83091   0.416 0.677294    \npoly(year, 2)1                     34.99408   16.39719   2.134 0.032830 *  \npoly(year, 2)2                      0.43033   15.23893   0.028 0.977471    \ntelicitytelic:poly(year, 2)1      -24.26891   11.32692  -2.143 0.032146 *  \ntelicitytelic:poly(year, 2)2      -26.95003   10.23544  -2.633 0.008463 ** \nanimacyyes:poly(year, 2)1          -0.69241   11.60801  -0.060 0.952435    \nanimacyyes:poly(year, 2)2         -21.17860   11.04972  -1.917 0.055281 .  \nfinitenessnonfin:poly(year, 2)1    -3.61152   10.77201  -0.335 0.737422    \nfinitenessnonfin:poly(year, 2)2     1.12707   10.20334   0.110 0.912044    \ncompound_tenseyes:poly(year, 2)1  -50.97581   21.38844  -2.383 0.017157 *  \ncompound_tenseyes:poly(year, 2)2  -15.53787   21.03734  -0.739 0.460159    \ngenreProsa:poly(year, 2)1         -25.45090   13.05124  -1.950 0.051167 .  \ngenreTeatro:poly(year, 2)1          9.43822   23.50149   0.402 0.687978    \ngenreProsa:poly(year, 2)2          19.16725   11.71397   1.636 0.101783    \ngenreTeatro:poly(year, 2)2         -3.53216   22.72296  -0.155 0.876471    \ncontrolyes:poly(year, 2)1          26.48886   13.09906   2.022 0.043156 *  \ncontrolyes:poly(year, 2)2          -5.82345   12.17452  -0.478 0.632415    \ncaus_use:poly(year, 2)1           -23.27363   23.60731  -0.986 0.324199    \ncaus_use:poly(year, 2)2            27.78972   22.08722   1.258 0.208326    \ntelicitytelic:languageSpanish       1.42182    0.50415   2.820 0.004799 ** \nanimacyyes:languageSpanish         -0.25458    0.45191  -0.563 0.573209    \nfinitenessnonfin:languageSpanish   -0.50993    0.28272  -1.804 0.071287 .  \ncompound_tenseyes:languageSpanish  -0.73725    0.57931  -1.273 0.203144    \ngenreProsa:languageSpanish          0.22742    0.33374   0.681 0.495596    \ngenreTeatro:languageSpanish        -0.68064    0.61002  -1.116 0.264522    \ncontrolyes:languageSpanish          0.75613    0.54580   1.385 0.165942    \nlanguageSpanish:reflprimingyes      0.78005    0.45937   1.698 0.089489 .  \nlanguageSpanish:caus_use           -2.57990    1.03880  -2.484 0.013008 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod2, mod1)\n\nData: data\nModels:\nmod2: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming * language + caus_use * language +      (1 | lemma) + (1 | author)\nmod1: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + reflpriming * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming *      language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod2   40 2602.4 2864.3 -1261.2   2522.4                     \nmod1   42 2606.1 2881.1 -1261.0   2522.1 0.3341  2     0.8462\n\n\nRemove finiteness:poly(year, 2)\n\n#finitenessnonfin:poly(year, 2)\n\nmod3&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ genre*language+\n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod3)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + caus_use * poly(year, 2) + telicity *  \n    language + animacy * language + finiteness * language + compound_tense *  \n    language + genre * language + control * language + reflpriming *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2598.5   2847.3  -1261.3   2522.5     5116 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.4888 -0.2150 -0.1140  0.2108 10.4773 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2158   0.4645  \n lemma  (Intercept) 5.1019   2.2587  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.3897     0.7465   0.522 0.601650    \ntelicitytelic                      -2.2000     0.4131  -5.325 1.01e-07 ***\nanimacyyes                          1.0553     0.3733   2.827 0.004699 ** \nfinitenessnonfin                    0.7240     0.2196   3.297 0.000978 ***\ncompound_tenseyes                   0.9458     0.3958   2.390 0.016869 *  \ngenreProsa                         -0.5931     0.2238  -2.650 0.008043 ** \ngenreTeatro                        -0.3449     0.3275  -1.053 0.292362    \ncontrolyes                         -1.2911     0.4442  -2.907 0.003652 ** \nlanguageSpanish                     0.1012     1.0223   0.099 0.921168    \nreflprimingyes                     -1.3284     0.3755  -3.538 0.000403 ***\ncaus_use                            0.3539     0.8302   0.426 0.669924    \npoly(year, 2)1                     34.3128    16.2423   2.113 0.034639 *  \npoly(year, 2)2                      0.5570    15.0018   0.037 0.970384    \ntelicitytelic:poly(year, 2)1      -23.8718    11.2669  -2.119 0.034112 *  \ntelicitytelic:poly(year, 2)2      -27.1609    10.1868  -2.666 0.007669 ** \nanimacyyes:poly(year, 2)1          -1.0686    11.5463  -0.093 0.926259    \nanimacyyes:poly(year, 2)2         -21.0320    11.0261  -1.907 0.056459 .  \ncompound_tenseyes:poly(year, 2)1  -50.6192    21.3407  -2.372 0.017694 *  \ncompound_tenseyes:poly(year, 2)2  -15.5197    20.9565  -0.741 0.458958    \ngenreProsa:poly(year, 2)1         -25.3404    13.0299  -1.945 0.051801 .  \ngenreTeatro:poly(year, 2)1          9.0890    23.4572   0.387 0.698407    \ngenreProsa:poly(year, 2)2          18.9435    11.6804   1.622 0.104842    \ngenreTeatro:poly(year, 2)2         -3.2956    22.6719  -0.145 0.884427    \ncontrolyes:poly(year, 2)1          26.3392    13.0922   2.012 0.044239 *  \ncontrolyes:poly(year, 2)2          -5.8408    12.1515  -0.481 0.630756    \ncaus_use:poly(year, 2)1           -23.3948    23.5607  -0.993 0.320732    \ncaus_use:poly(year, 2)2            28.1115    22.0345   1.276 0.202029    \ntelicitytelic:languageSpanish       1.4244     0.5039   2.827 0.004701 ** \nanimacyyes:languageSpanish         -0.2559     0.4518  -0.566 0.571190    \nfinitenessnonfin:languageSpanish   -0.5070     0.2811  -1.803 0.071338 .  \ncompound_tenseyes:languageSpanish  -0.7335     0.5787  -1.267 0.205017    \ngenreProsa:languageSpanish          0.2259     0.3335   0.677 0.498144    \ngenreTeatro:languageSpanish        -0.6852     0.6098  -1.124 0.261139    \ncontrolyes:languageSpanish          0.7568     0.5455   1.387 0.165320    \nlanguageSpanish:reflprimingyes      0.7797     0.4588   1.699 0.089247 .  \nlanguageSpanish:caus_use           -2.5936     1.0372  -2.501 0.012397 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod2, mod3)\n\nData: data\nModels:\nmod3: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod2: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + finiteness * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming * language + caus_use * language +      (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod3   38 2598.5 2847.3 -1261.3   2522.5                     \nmod2   40 2602.4 2864.3 -1261.2   2522.4 0.1194  2      0.942\n\n\nRemove genre:language\n\n#genre:language\n\nmod4&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ animacy*language+ finiteness*language+ compound_tense*language+ \n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod4)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + caus_use * poly(year, 2) + telicity *  \n    language + animacy * language + finiteness * language + compound_tense *  \n    language + control * language + reflpriming * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2597.4   2833.1  -1262.7   2525.4     5118 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.9980 -0.2166 -0.1154  0.2084 10.3774 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2122   0.4606  \n lemma  (Intercept) 5.1300   2.2650  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.35409    0.74055   0.478 0.632545    \ntelicitytelic                      -2.19375    0.41264  -5.316 1.06e-07 ***\nanimacyyes                          1.06842    0.37199   2.872 0.004077 ** \nfinitenessnonfin                    0.71006    0.21862   3.248 0.001162 ** \ncompound_tenseyes                   0.94504    0.39507   2.392 0.016753 *  \ngenreProsa                         -0.47683    0.17406  -2.739 0.006155 ** \ngenreTeatro                        -0.51273    0.28431  -1.803 0.071328 .  \ncontrolyes                         -1.28346    0.44229  -2.902 0.003710 ** \nlanguageSpanish                     0.19869    0.98468   0.202 0.840087    \nreflprimingyes                     -1.33386    0.37554  -3.552 0.000382 ***\ncaus_use                            0.32847    0.82890   0.396 0.691901    \npoly(year, 2)1                     34.68193   16.16483   2.146 0.031911 *  \npoly(year, 2)2                      1.59954   14.84409   0.108 0.914189    \ntelicitytelic:poly(year, 2)1      -24.26644   11.24722  -2.158 0.030963 *  \ntelicitytelic:poly(year, 2)2      -27.65528   10.18061  -2.716 0.006598 ** \nanimacyyes:poly(year, 2)1          -0.02539   11.45594  -0.002 0.998232    \nanimacyyes:poly(year, 2)2         -20.85659   10.97189  -1.901 0.057314 .  \ncompound_tenseyes:poly(year, 2)1  -49.52388   21.36581  -2.318 0.020455 *  \ncompound_tenseyes:poly(year, 2)2  -13.14826   20.97386  -0.627 0.530733    \ngenreProsa:poly(year, 2)1         -25.82030   12.91071  -2.000 0.045510 *  \ngenreTeatro:poly(year, 2)1          3.15715   23.34743   0.135 0.892434    \ngenreProsa:poly(year, 2)2          18.19720   11.47024   1.586 0.112633    \ngenreTeatro:poly(year, 2)2        -11.42270   22.23109  -0.514 0.607380    \ncontrolyes:poly(year, 2)1          25.41198   12.97540   1.958 0.050174 .  \ncontrolyes:poly(year, 2)2          -5.74013   12.08826  -0.475 0.634893    \ncaus_use:poly(year, 2)1           -23.14298   23.51364  -0.984 0.324999    \ncaus_use:poly(year, 2)2            28.13574   22.02434   1.277 0.201431    \ntelicitytelic:languageSpanish       1.45617    0.50329   2.893 0.003812 ** \nanimacyyes:languageSpanish         -0.28837    0.45039  -0.640 0.521997    \nfinitenessnonfin:languageSpanish   -0.48929    0.28033  -1.745 0.080914 .  \ncompound_tenseyes:languageSpanish  -0.77813    0.58036  -1.341 0.179994    \ncontrolyes:languageSpanish          0.75467    0.54413   1.387 0.165466    \nlanguageSpanish:reflprimingyes      0.79330    0.45855   1.730 0.083623 .  \nlanguageSpanish:caus_use           -2.57614    1.03571  -2.487 0.012871 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod4, mod3)\n\nData: data\nModels:\nmod4: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod3: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + genre * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod4   36 2597.4 2833.1 -1262.7   2525.4                     \nmod3   38 2598.5 2847.3 -1261.3   2522.5 2.8745  2     0.2376\n\n\nRemove animacy:language\n\n# animacy:language\n\nmod5&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + caus_use*poly(year,2) +\n              #language\n              telicity*language+ finiteness*language+ compound_tense*language+ \n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod5)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + caus_use * poly(year, 2) + telicity *  \n    language + finiteness * language + compound_tense * language +  \n    control * language + reflpriming * language + caus_use *  \n    language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2595.8   2825.0  -1262.9   2525.8     5119 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.3056 -0.2175 -0.1155  0.2098 10.4158 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2139   0.4625  \n lemma  (Intercept) 5.1201   2.2628  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.3748     0.7392   0.507 0.612124    \ntelicitytelic                      -2.1911     0.4114  -5.326 1.01e-07 ***\nanimacyyes                          0.8713     0.2077   4.195 2.73e-05 ***\nfinitenessnonfin                    0.7091     0.2183   3.248 0.001163 ** \ncompound_tenseyes                   0.9428     0.3944   2.391 0.016825 *  \ngenreProsa                         -0.4781     0.1743  -2.743 0.006079 ** \ngenreTeatro                        -0.5118     0.2844  -1.800 0.071913 .  \ncontrolyes                         -1.1045     0.3437  -3.214 0.001309 ** \nlanguageSpanish                     0.1770     0.9835   0.180 0.857194    \nreflprimingyes                     -1.3372     0.3749  -3.567 0.000361 ***\ncaus_use                            0.3403     0.8281   0.411 0.681093    \npoly(year, 2)1                     34.7186    16.1796   2.146 0.031887 *  \npoly(year, 2)2                      1.7792    14.8574   0.120 0.904680    \ntelicitytelic:poly(year, 2)1      -23.9476    11.2428  -2.130 0.033168 *  \ntelicitytelic:poly(year, 2)2      -27.6427    10.1808  -2.715 0.006624 ** \nanimacyyes:poly(year, 2)1          -0.2325    11.4751  -0.020 0.983836    \nanimacyyes:poly(year, 2)2         -21.7332    10.9093  -1.992 0.046353 *  \ncompound_tenseyes:poly(year, 2)1  -49.6692    21.3738  -2.324 0.020134 *  \ncompound_tenseyes:poly(year, 2)2  -12.8508    20.9870  -0.612 0.540325    \ngenreProsa:poly(year, 2)1         -25.9256    12.9227  -2.006 0.044833 *  \ngenreTeatro:poly(year, 2)1          2.8839    23.3427   0.124 0.901675    \ngenreProsa:poly(year, 2)2          18.1318    11.4796   1.579 0.114226    \ngenreTeatro:poly(year, 2)2        -11.1587    22.2394  -0.502 0.615841    \ncontrolyes:poly(year, 2)1          25.3565    13.0019   1.950 0.051150 .  \ncontrolyes:poly(year, 2)2          -5.0792    12.0748  -0.421 0.674015    \ncaus_use:poly(year, 2)1           -23.3069    23.5053  -0.992 0.321412    \ncaus_use:poly(year, 2)2            28.1811    22.0188   1.280 0.200593    \ntelicitytelic:languageSpanish       1.4495     0.5023   2.886 0.003904 ** \nfinitenessnonfin:languageSpanish   -0.4902     0.2803  -1.749 0.080301 .  \ncompound_tenseyes:languageSpanish  -0.7803     0.5800  -1.345 0.178544    \ncontrolyes:languageSpanish          0.4948     0.3635   1.361 0.173460    \nlanguageSpanish:reflprimingyes      0.7956     0.4582   1.736 0.082549 .  \nlanguageSpanish:caus_use           -2.5960     1.0351  -2.508 0.012140 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod4, mod5)\n\nData: data\nModels:\nmod5: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod4: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + animacy * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod5   35 2595.8 2825.0 -1262.9   2525.8                     \nmod4   36 2597.4 2833.1 -1262.7   2525.4 0.4225  1     0.5157\n\n\nRemove caus_use:poly(year, 2)\n\n#caus_use:poly(year, 2)\n\nmod6&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ finiteness*language+ compound_tense*language+ \n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod6)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + finiteness *  \n    language + compound_tense * language + control * language +  \n    reflpriming * language + caus_use * language + (1 | lemma) +  \n    (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2594.4   2810.5  -1264.2   2528.4     5121 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.6787 -0.2171 -0.1152  0.2085  8.9834 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1848   0.4299  \n lemma  (Intercept) 5.1839   2.2768  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.1967     0.7281   0.270 0.787024    \ntelicitytelic                      -2.1394     0.4068  -5.260 1.44e-07 ***\nanimacyyes                          0.8684     0.2072   4.191 2.77e-05 ***\nfinitenessnonfin                    0.7148     0.2173   3.290 0.001002 ** \ncompound_tenseyes                   0.9234     0.3924   2.353 0.018616 *  \ngenreProsa                         -0.4699     0.1720  -2.731 0.006305 ** \ngenreTeatro                        -0.5216     0.2817  -1.852 0.064054 .  \ncontrolyes                         -1.1087     0.3419  -3.243 0.001185 ** \nlanguageSpanish                     0.2651     0.9820   0.270 0.787234    \nreflprimingyes                     -1.3198     0.3716  -3.552 0.000383 ***\ncaus_use                            0.5812     0.7949   0.731 0.464651    \npoly(year, 2)1                     28.2103    13.6509   2.067 0.038777 *  \npoly(year, 2)2                     12.2720    12.0529   1.018 0.308591    \ntelicitytelic:poly(year, 2)1      -29.2756    10.4086  -2.813 0.004914 ** \ntelicitytelic:poly(year, 2)2      -22.2533     9.4197  -2.362 0.018157 *  \nanimacyyes:poly(year, 2)1           0.3898    11.4405   0.034 0.972819    \nanimacyyes:poly(year, 2)2         -22.2290    10.8756  -2.044 0.040960 *  \ncompound_tenseyes:poly(year, 2)1  -44.7148    21.2026  -2.109 0.034951 *  \ncompound_tenseyes:poly(year, 2)2  -17.0961    20.8182  -0.821 0.411525    \ngenreProsa:poly(year, 2)1         -27.5889    12.7430  -2.165 0.030386 *  \ngenreTeatro:poly(year, 2)1          3.8331    23.1471   0.166 0.868474    \ngenreProsa:poly(year, 2)2          18.9237    11.3599   1.666 0.095747 .  \ngenreTeatro:poly(year, 2)2        -13.3958    22.0805  -0.607 0.544063    \ncontrolyes:poly(year, 2)1          27.2153    12.8699   2.115 0.034459 *  \ncontrolyes:poly(year, 2)2          -6.1055    12.0223  -0.508 0.611560    \ntelicitytelic:languageSpanish       1.3834     0.4966   2.786 0.005341 ** \nfinitenessnonfin:languageSpanish   -0.4947     0.2790  -1.773 0.076250 .  \ncompound_tenseyes:languageSpanish  -0.7934     0.5774  -1.374 0.169438    \ncontrolyes:languageSpanish          0.4963     0.3620   1.371 0.170362    \nlanguageSpanish:reflprimingyes      0.7952     0.4556   1.745 0.080917 .  \nlanguageSpanish:caus_use           -2.6592     1.0209  -2.605 0.009193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod6, mod5)\n\nData: data\nModels:\nmod6: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod5: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + caus_use * poly(year, 2) + telicity * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod6   33 2594.4 2810.5 -1264.2   2528.4                     \nmod5   35 2595.8 2825.0 -1262.9   2525.8 2.5807  2     0.2752\n\n\nRemove compound_tense:language\n\n#compound_tense:language\n\nmod7&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ finiteness*language+ \n              control*language+ reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod7)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + finiteness *  \n    language + control * language + reflpriming * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2594.3   2803.9  -1265.2   2530.3     5122 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.6895 -0.2169 -0.1160  0.2087  9.1398 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1854   0.4306  \n lemma  (Intercept) 5.1974   2.2798  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2183     0.7280   0.300 0.764268    \ntelicitytelic                     -2.1272     0.4064  -5.234 1.66e-07 ***\nanimacyyes                         0.8649     0.2070   4.179 2.93e-05 ***\nfinitenessnonfin                   0.6857     0.2156   3.181 0.001470 ** \ncompound_tenseyes                  0.5360     0.2860   1.874 0.060882 .  \ngenreProsa                        -0.4599     0.1718  -2.677 0.007429 ** \ngenreTeatro                       -0.5151     0.2822  -1.826 0.067912 .  \ncontrolyes                        -1.0980     0.3414  -3.216 0.001298 ** \nlanguageSpanish                    0.2115     0.9819   0.215 0.829487    \nreflprimingyes                    -1.3198     0.3716  -3.552 0.000383 ***\ncaus_use                           0.5696     0.7929   0.718 0.472487    \npoly(year, 2)1                    27.9517    13.6427   2.049 0.040478 *  \npoly(year, 2)2                    12.0736    12.0556   1.001 0.316587    \ntelicitytelic:poly(year, 2)1     -29.0550    10.4027  -2.793 0.005222 ** \ntelicitytelic:poly(year, 2)2     -22.1117     9.4262  -2.346 0.018988 *  \nanimacyyes:poly(year, 2)1         -0.6015    11.4139  -0.053 0.957971    \nanimacyyes:poly(year, 2)2        -21.9443    10.8817  -2.017 0.043735 *  \ncompound_tenseyes:poly(year, 2)1 -46.9257    21.6675  -2.166 0.030332 *  \ncompound_tenseyes:poly(year, 2)2 -25.7056    20.3428  -1.264 0.206365    \ngenreProsa:poly(year, 2)1        -27.2029    12.7430  -2.135 0.032783 *  \ngenreTeatro:poly(year, 2)1         5.0229    23.1749   0.217 0.828411    \ngenreProsa:poly(year, 2)2         18.8820    11.3753   1.660 0.096930 .  \ngenreTeatro:poly(year, 2)2       -12.5218    22.0669  -0.567 0.570410    \ncontrolyes:poly(year, 2)1         27.8244    12.8710   2.162 0.030635 *  \ncontrolyes:poly(year, 2)2         -6.0172    12.0339  -0.500 0.617060    \ntelicitytelic:languageSpanish      1.3676     0.4969   2.752 0.005915 ** \nfinitenessnonfin:languageSpanish  -0.4503     0.2767  -1.627 0.103640    \ncontrolyes:languageSpanish         0.4967     0.3618   1.373 0.169839    \nlanguageSpanish:reflprimingyes     0.7945     0.4556   1.744 0.081165 .  \nlanguageSpanish:caus_use          -2.6388     1.0199  -2.587 0.009670 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod6, mod7)\n\nData: data\nModels:\nmod7: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod6: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + compound_tense * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod7   32 2594.3 2803.9 -1265.2   2530.3                     \nmod6   33 2594.4 2810.5 -1264.2   2528.4 1.9428  1     0.1634\n\n\nRemove control:language\n\n#control:language  \n\nmod8&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ finiteness*language+ \n              reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod8)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + finiteness *  \n    language + reflpriming * language + caus_use * language +  \n    (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2594.3   2797.3  -1266.2   2532.3     5123 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.2338 -0.2157 -0.1158  0.2070  9.0616 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1933   0.4396  \n lemma  (Intercept) 5.2888   2.2997  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2415     0.7327   0.330 0.741689    \ntelicitytelic                     -2.2645     0.3970  -5.704 1.17e-08 ***\nanimacyyes                         0.8640     0.2074   4.166 3.10e-05 ***\nfinitenessnonfin                   0.6564     0.2144   3.061 0.002205 ** \ncompound_tenseyes                  0.5214     0.2849   1.830 0.067220 .  \ngenreProsa                        -0.4658     0.1722  -2.705 0.006831 ** \ngenreTeatro                       -0.5328     0.2820  -1.890 0.058821 .  \ncontrolyes                        -0.7786     0.2535  -3.071 0.002135 ** \nlanguageSpanish                    0.2344     0.9879   0.237 0.812428    \nreflprimingyes                    -1.3302     0.3732  -3.564 0.000365 ***\ncaus_use                           0.6228     0.7957   0.783 0.433735    \npoly(year, 2)1                    27.4667    13.6730   2.009 0.044556 *  \npoly(year, 2)2                    12.0219    12.0801   0.995 0.319648    \ntelicitytelic:poly(year, 2)1     -27.9571    10.3842  -2.692 0.007096 ** \ntelicitytelic:poly(year, 2)2     -22.3772     9.4331  -2.372 0.017683 *  \nanimacyyes:poly(year, 2)1         -0.5715    11.4250  -0.050 0.960103    \nanimacyyes:poly(year, 2)2        -21.5709    10.8929  -1.980 0.047673 *  \ncompound_tenseyes:poly(year, 2)1 -46.3850    21.6474  -2.143 0.032133 *  \ncompound_tenseyes:poly(year, 2)2 -25.4520    20.3183  -1.253 0.210327    \ngenreProsa:poly(year, 2)1        -27.8006    12.7744  -2.176 0.029535 *  \ngenreTeatro:poly(year, 2)1         3.7350    23.1890   0.161 0.872039    \ngenreProsa:poly(year, 2)2         18.5382    11.3900   1.628 0.103611    \ngenreTeatro:poly(year, 2)2       -12.0271    22.0881  -0.545 0.586095    \ncontrolyes:poly(year, 2)1         26.1034    12.8150   2.037 0.041656 *  \ncontrolyes:poly(year, 2)2         -5.3265    12.0252  -0.443 0.657804    \ntelicitytelic:languageSpanish      1.5127     0.4881   3.099 0.001941 ** \nfinitenessnonfin:languageSpanish  -0.4051     0.2748  -1.474 0.140354    \nlanguageSpanish:reflprimingyes     0.8058     0.4569   1.764 0.077783 .  \nlanguageSpanish:caus_use          -2.6849     1.0219  -2.627 0.008606 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod8, mod7)\n\nData: data\nModels:\nmod8: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod7: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + control * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod8   31 2594.3 2797.3 -1266.2   2532.3                     \nmod7   32 2594.3 2803.9 -1265.2   2530.3 1.9593  1     0.1616\n\n\nRemove finiteness:language\n\n#finiteness:language \n\nmod9&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ \n              reflpriming*language+ caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod9)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + reflpriming *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2594.5   2791.0  -1267.3   2534.5     5124 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.4590 -0.2169 -0.1156  0.2064  9.3486 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1889   0.4347  \n lemma  (Intercept) 5.2794   2.2977  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2871     0.7308   0.393 0.694388    \ntelicitytelic                     -2.2640     0.3951  -5.730 1.01e-08 ***\nanimacyyes                         0.8587     0.2072   4.144 3.41e-05 ***\nfinitenessnonfin                   0.4133     0.1377   3.002 0.002681 ** \ncompound_tenseyes                  0.5057     0.2841   1.780 0.075048 .  \ngenreProsa                        -0.4491     0.1712  -2.623 0.008719 ** \ngenreTeatro                       -0.5080     0.2805  -1.811 0.070142 .  \ncontrolyes                        -0.7727     0.2533  -3.051 0.002284 ** \nlanguageSpanish                    0.1385     0.9844   0.141 0.888128    \nreflprimingyes                    -1.3021     0.3686  -3.533 0.000412 ***\ncaus_use                           0.6278     0.7937   0.791 0.428996    \npoly(year, 2)1                    26.8709    13.6244   1.972 0.048579 *  \npoly(year, 2)2                    11.3566    12.0417   0.943 0.345629    \ntelicitytelic:poly(year, 2)1     -27.6962    10.3742  -2.670 0.007591 ** \ntelicitytelic:poly(year, 2)2     -22.3577     9.4265  -2.372 0.017702 *  \nanimacyyes:poly(year, 2)1         -0.9065    11.4286  -0.079 0.936782    \nanimacyyes:poly(year, 2)2        -21.3859    10.8939  -1.963 0.049635 *  \ncompound_tenseyes:poly(year, 2)1 -45.8172    21.5738  -2.124 0.033691 *  \ncompound_tenseyes:poly(year, 2)2 -24.5888    20.2270  -1.216 0.224121    \ngenreProsa:poly(year, 2)1        -27.4022    12.7236  -2.154 0.031268 *  \ngenreTeatro:poly(year, 2)1         2.7554    23.0801   0.119 0.904970    \ngenreProsa:poly(year, 2)2         18.9378    11.3598   1.667 0.095497 .  \ngenreTeatro:poly(year, 2)2       -11.3645    21.9875  -0.517 0.605253    \ncontrolyes:poly(year, 2)1         26.4304    12.8087   2.063 0.039068 *  \ncontrolyes:poly(year, 2)2         -4.8303    12.0187  -0.402 0.687760    \ntelicitytelic:languageSpanish      1.5194     0.4864   3.124 0.001785 ** \nlanguageSpanish:reflprimingyes     0.7779     0.4533   1.716 0.086170 .  \nlanguageSpanish:caus_use          -2.7015     1.0207  -2.647 0.008129 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod9, mod8)\n\nData: data\nModels:\nmod9: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\nmod8: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + finiteness * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod9   30 2594.5 2791.0 -1267.3   2534.5                     \nmod8   31 2594.3 2797.3 -1266.2   2532.3 2.2273  1     0.1356\n\n\nRemove language:reflpriming. This only marginally affects the fit, so we remove it for the sake of parsimony.\n\n#language:reflpriming\n\nmod10&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n              control + language + reflpriming + caus_use + poly(year,2) +\n              #year\n              telicity*poly(year,2) + animacy*poly(year,2) + compound_tense*poly(year,2) + \n              genre*poly(year,2) +\n              control*poly(year,2) + \n              #language\n              telicity*language+ \n              caus_use*language+\n              #random\n              (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod10)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    compound_tense * poly(year, 2) + genre * poly(year, 2) +  \n    control * poly(year, 2) + telicity * language + caus_use *  \n    language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2595.7   2785.5  -1268.8   2537.7     5125 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.2456 -0.2189 -0.1147  0.2083 10.3890 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.195    0.4416  \n lemma  (Intercept) 5.212    2.2831  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2591     0.7274   0.356 0.721720    \ntelicitytelic                     -2.2369     0.3931  -5.691 1.27e-08 ***\nanimacyyes                         0.8667     0.2074   4.178 2.94e-05 ***\nfinitenessnonfin                   0.4087     0.1376   2.971 0.002967 ** \ncompound_tenseyes                  0.4930     0.2841   1.735 0.082696 .  \ngenreProsa                        -0.4533     0.1712  -2.647 0.008119 ** \ngenreTeatro                       -0.5017     0.2807  -1.787 0.073881 .  \ncontrolyes                        -0.7834     0.2534  -3.091 0.001994 ** \nlanguageSpanish                    0.1926     0.9796   0.197 0.844167    \nreflprimingyes                    -0.7965     0.2131  -3.738 0.000185 ***\ncaus_use                           0.5840     0.7903   0.739 0.459962    \npoly(year, 2)1                    28.0883    13.5922   2.067 0.038781 *  \npoly(year, 2)2                    12.0286    12.0347   0.999 0.317554    \ntelicitytelic:poly(year, 2)1     -28.9759    10.3656  -2.795 0.005184 ** \ntelicitytelic:poly(year, 2)2     -22.1901     9.4332  -2.352 0.018655 *  \nanimacyyes:poly(year, 2)1         -1.5556    11.4412  -0.136 0.891849    \nanimacyyes:poly(year, 2)2        -21.5509    10.9011  -1.977 0.048048 *  \ncompound_tenseyes:poly(year, 2)1 -44.3471    21.5677  -2.056 0.039765 *  \ncompound_tenseyes:poly(year, 2)2 -24.5765    20.2619  -1.213 0.225153    \ngenreProsa:poly(year, 2)1        -28.2452    12.6992  -2.224 0.026137 *  \ngenreTeatro:poly(year, 2)1         1.5550    23.1286   0.067 0.946395    \ngenreProsa:poly(year, 2)2         18.3940    11.3567   1.620 0.105306    \ngenreTeatro:poly(year, 2)2       -12.4002    22.0086  -0.563 0.573147    \ncontrolyes:poly(year, 2)1         27.6178    12.8266   2.153 0.031305 *  \ncontrolyes:poly(year, 2)2         -4.7322    12.0577  -0.392 0.694719    \ntelicitytelic:languageSpanish      1.4819     0.4847   3.058 0.002231 ** \nlanguageSpanish:caus_use          -2.6242     1.0181  -2.578 0.009952 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod9, mod10) #marginal=remove\n\nData: data\nModels:\nmod10: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod9: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + reflpriming * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nmod10   29 2595.7 2785.5 -1268.8   2537.7                       \nmod9    30 2594.5 2791.0 -1267.3   2534.5 3.1252  1    0.07709 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemove compound_tense:poly(year, 2). ANOVA shows it is not ok to remove, as this significantly affects the fit.\n\n#compound_tense:poly(year, 2)2\n\nmod11&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2) + animacy*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod11)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + animacy * poly(year, 2) +  \n    genre * poly(year, 2) + control * poly(year, 2) + telicity *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2599.9   2776.7  -1273.0   2545.9     5127 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.1150 -0.2191 -0.1159  0.2085 10.1111 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1987   0.4458  \n lemma  (Intercept) 5.2101   2.2826  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     0.2731     0.7262   0.376 0.706892    \ntelicitytelic                  -2.2468     0.3928  -5.719 1.07e-08 ***\nanimacyyes                      0.8613     0.2071   4.158 3.21e-05 ***\nfinitenessnonfin                0.3997     0.1371   2.915 0.003552 ** \ncompound_tenseyes               0.2537     0.2823   0.899 0.368776    \ngenreProsa                     -0.4490     0.1707  -2.631 0.008513 ** \ngenreTeatro                    -0.5054     0.2800  -1.805 0.071014 .  \ncontrolyes                     -0.7598     0.2530  -3.003 0.002670 ** \nlanguageSpanish                 0.2007     0.9786   0.205 0.837503    \nreflprimingyes                 -0.7930     0.2127  -3.728 0.000193 ***\ncaus_use                        0.5923     0.7859   0.754 0.451007    \npoly(year, 2)1                 27.3278    13.5538   2.016 0.043774 *  \npoly(year, 2)2                 11.5955    12.0030   0.966 0.334020    \ntelicitytelic:poly(year, 2)1  -30.3751    10.2889  -2.952 0.003155 ** \ntelicitytelic:poly(year, 2)2  -22.7445     9.3581  -2.430 0.015080 *  \nanimacyyes:poly(year, 2)1      -1.1483    11.4135  -0.101 0.919859    \nanimacyyes:poly(year, 2)2     -21.0048    10.8748  -1.932 0.053419 .  \ngenreProsa:poly(year, 2)1     -29.1223    12.6703  -2.298 0.021536 *  \ngenreTeatro:poly(year, 2)1     -0.5833    23.0994  -0.025 0.979853    \ngenreProsa:poly(year, 2)2      17.0467    11.3134   1.507 0.131869    \ngenreTeatro:poly(year, 2)2    -15.8250    21.9098  -0.722 0.470124    \ncontrolyes:poly(year, 2)1      28.6520    12.7984   2.239 0.025175 *  \ncontrolyes:poly(year, 2)2      -4.3995    12.0422  -0.365 0.714859    \ntelicitytelic:languageSpanish   1.4747     0.4837   3.049 0.002296 ** \nlanguageSpanish:caus_use       -2.6588     1.0144  -2.621 0.008765 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod11, mod10) # not ok\n\nData: data\nModels:\nmod11: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod10: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance Chisq Df Pr(&gt;Chisq)  \nmod11   27 2599.9 2776.7 -1273.0   2545.9                      \nmod10   29 2595.7 2785.5 -1268.8   2537.7 8.265  2    0.01604 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBack to mod10, remove animacy:poly(year, 2), which only marginally affects the fit, so we remove it for the sake of parsimony.\n\n#animacy:poly(year, 2)1 \n\nmod12&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod12)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + telicity *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2595.8   2772.6  -1270.9   2541.8     5127 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-11.6137  -0.2193  -0.1148   0.2067   9.9198 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1994   0.4466  \n lemma  (Intercept) 5.2094   2.2824  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2844     0.7271   0.391 0.695712    \ntelicitytelic                     -2.2402     0.3930  -5.699 1.20e-08 ***\nanimacyyes                         0.8563     0.2069   4.140 3.48e-05 ***\nfinitenessnonfin                   0.3961     0.1372   2.887 0.003887 ** \ncompound_tenseyes                  0.4842     0.2833   1.709 0.087495 .  \ngenreProsa                        -0.4371     0.1708  -2.559 0.010490 *  \ngenreTeatro                       -0.4871     0.2813  -1.732 0.083352 .  \ncontrolyes                        -0.7756     0.2533  -3.062 0.002199 ** \nlanguageSpanish                    0.1671     0.9793   0.171 0.864520    \nreflprimingyes                    -0.7960     0.2118  -3.759 0.000171 ***\ncaus_use                           0.5272     0.7913   0.666 0.505245    \npoly(year, 2)1                    29.4819    13.4208   2.197 0.028039 *  \npoly(year, 2)2                     6.4140    11.6002   0.553 0.580316    \ntelicitytelic:poly(year, 2)1     -29.9739    10.2595  -2.922 0.003483 ** \ntelicitytelic:poly(year, 2)2     -24.9774     9.3679  -2.666 0.007670 ** \ncompound_tenseyes:poly(year, 2)1 -43.9612    21.3327  -2.061 0.039328 *  \ncompound_tenseyes:poly(year, 2)2 -24.6607    19.9895  -1.234 0.217321    \ngenreProsa:poly(year, 2)1        -30.5676    12.6635  -2.414 0.015786 *  \ngenreTeatro:poly(year, 2)1        -0.3106    23.2091  -0.013 0.989322    \ngenreProsa:poly(year, 2)2         20.2971    11.2061   1.811 0.070101 .  \ngenreTeatro:poly(year, 2)2       -11.2673    22.1161  -0.509 0.610428    \ncontrolyes:poly(year, 2)1         27.1658     9.8018   2.772 0.005580 ** \ncontrolyes:poly(year, 2)2        -19.6613     9.3335  -2.107 0.035159 *  \ntelicitytelic:languageSpanish      1.4676     0.4840   3.032 0.002429 ** \nlanguageSpanish:caus_use          -2.5584     1.0196  -2.509 0.012101 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod12, mod10) #marginal=remove\n\nData: data\nModels:\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod10: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + animacy * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod12   27 2595.8 2772.6 -1270.9   2541.8                     \nmod10   29 2595.7 2785.5 -1268.8   2537.7 4.1168  2     0.1277\n\n\nRemove genre:poly(year, 2). This significantly affects the fit.\n\n#genreTeatro:poly(year, 2)1 \n\nmod13&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\n#summary(mod13)\nanova(mod12, mod13) #not ok\n\nData: data\nModels:\nmod13: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)   \nmod13   23 2601.4 2752.0 -1277.7   2555.4                        \nmod12   27 2595.8 2772.6 -1270.9   2541.8 13.629  4   0.008578 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#model 12 final (not possible to remove anything else)\n\nBack to mod12, we could try to remove the simple terms that are not involved in interactions: finiteness, reflpriming, animacy: however these are all highly significant in the model summary of mod12 and removing them significantly affects the fit, as shown in the code below:\n\n# remove finiteness\n\nmod14&lt;-glmer(coding ~ telicity + animacy +  compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod14)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + compound_tense + genre + control +  \n    language + reflpriming + caus_use + poly(year, 2) + telicity *  \n    poly(year, 2) + compound_tense * poly(year, 2) + genre *  \n    poly(year, 2) + control * poly(year, 2) + telicity * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2602.4   2772.6  -1275.2   2550.4     5128 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-11.7703  -0.2226  -0.1158   0.2066   9.5674 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.184    0.4289  \n lemma  (Intercept) 5.220    2.2847  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.3630     0.7264   0.500 0.617290    \ntelicitytelic                     -2.2823     0.3915  -5.829 5.57e-09 ***\nanimacyyes                         0.8665     0.2057   4.213 2.52e-05 ***\ncompound_tenseyes                  0.4001     0.2818   1.420 0.155740    \ngenreProsa                        -0.4195     0.1695  -2.475 0.013317 *  \ngenreTeatro                       -0.4640     0.2793  -1.661 0.096641 .  \ncontrolyes                        -0.7349     0.2518  -2.918 0.003521 ** \nlanguageSpanish                    0.1661     0.9795   0.170 0.865334    \nreflprimingyes                    -0.7835     0.2100  -3.731 0.000191 ***\ncaus_use                           0.5710     0.7901   0.723 0.469831    \npoly(year, 2)1                    28.5120    13.3338   2.138 0.032490 *  \npoly(year, 2)2                     5.9569    11.5094   0.518 0.604758    \ntelicitytelic:poly(year, 2)1     -29.1353    10.2344  -2.847 0.004416 ** \ntelicitytelic:poly(year, 2)2     -25.2423     9.3258  -2.707 0.006795 ** \ncompound_tenseyes:poly(year, 2)1 -44.7673    21.2548  -2.106 0.035185 *  \ncompound_tenseyes:poly(year, 2)2 -22.0843    19.8380  -1.113 0.265609    \ngenreProsa:poly(year, 2)1        -28.5563    12.5449  -2.276 0.022826 *  \ngenreTeatro:poly(year, 2)1        -1.6448    23.0160  -0.071 0.943028    \ngenreProsa:poly(year, 2)2         19.4068    11.1233   1.745 0.081037 .  \ngenreTeatro:poly(year, 2)2       -10.1599    21.8823  -0.464 0.642435    \ncontrolyes:poly(year, 2)1         27.3656     9.7869   2.796 0.005172 ** \ncontrolyes:poly(year, 2)2        -19.8295     9.3212  -2.127 0.033391 *  \ntelicitytelic:languageSpanish      1.4977     0.4829   3.102 0.001925 ** \nlanguageSpanish:caus_use          -2.6191     1.0161  -2.578 0.009951 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod12, mod14) #not ok\n\nData: data\nModels:\nmod14: coding ~ telicity + animacy + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance Chisq Df Pr(&gt;Chisq)   \nmod14   26 2602.4 2772.6 -1275.2   2550.4                       \nmod12   27 2595.8 2772.6 -1270.9   2541.8 8.622  1   0.003321 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# remove animacy\n\nmod15&lt;-glmer(coding ~ telicity + finiteness + compound_tense + genre +\n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod15)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + finiteness + compound_tense + genre + control +  \n    language + reflpriming + caus_use + poly(year, 2) + telicity *  \n    poly(year, 2) + compound_tense * poly(year, 2) + genre *  \n    poly(year, 2) + control * poly(year, 2) + telicity * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2611.5   2781.7  -1279.7   2559.5     5128 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.6668 -0.2219 -0.1167  0.2019  9.8757 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1874   0.4329  \n lemma  (Intercept) 5.4416   2.3327  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.405477   0.732214   0.554 0.579738    \ntelicitytelic                     -2.228758   0.387872  -5.746 9.13e-09 ***\nfinitenessnonfin                   0.406703   0.136069   2.989 0.002799 ** \ncompound_tenseyes                  0.496729   0.281969   1.762 0.078129 .  \ngenreProsa                        -0.467873   0.169962  -2.753 0.005909 ** \ngenreTeatro                       -0.485781   0.279701  -1.737 0.082424 .  \ncontrolyes                        -0.002561   0.174363  -0.015 0.988283    \nlanguageSpanish                    0.074091   0.988589   0.075 0.940258    \nreflprimingyes                    -0.801201   0.210666  -3.803 0.000143 ***\ncaus_use                           0.542181   0.788905   0.687 0.491920    \npoly(year, 2)1                    29.190220  13.393391   2.179 0.029298 *  \npoly(year, 2)2                     6.023283  11.488203   0.524 0.600069    \ntelicitytelic:poly(year, 2)1     -31.205569  10.219843  -3.053 0.002262 ** \ntelicitytelic:poly(year, 2)2     -22.406518   9.289687  -2.412 0.015866 *  \ncompound_tenseyes:poly(year, 2)1 -42.819094  21.172315  -2.022 0.043134 *  \ncompound_tenseyes:poly(year, 2)2 -24.675497  19.914011  -1.239 0.215308    \ngenreProsa:poly(year, 2)1        -28.657149  12.580072  -2.278 0.022728 *  \ngenreTeatro:poly(year, 2)1         0.936845  23.111253   0.041 0.967666    \ngenreProsa:poly(year, 2)2         19.185881  11.070621   1.733 0.083088 .  \ngenreTeatro:poly(year, 2)2       -12.746166  21.958101  -0.580 0.561593    \ncontrolyes:poly(year, 2)1         28.008451   9.745994   2.874 0.004055 ** \ncontrolyes:poly(year, 2)2        -20.110222   9.269633  -2.169 0.030047 *  \ntelicitytelic:languageSpanish      1.499075   0.480267   3.121 0.001800 ** \nlanguageSpanish:caus_use          -2.474567   1.013529  -2.442 0.014625 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod12, mod15) #not ok\n\nData: data\nModels:\nmod15: coding ~ telicity + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmod15   26 2611.5 2781.7 -1279.8   2559.5                         \nmod12   27 2595.8 2772.6 -1270.9   2541.8 17.718  1  2.562e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# remove reflpriming\n\nmod16&lt;-glmer(coding ~ telicity + finiteness + animacy + compound_tense + genre +\n               control + language +  caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2)+ compound_tense*poly(year,2) + \n               genre*poly(year,2) +\n               control*poly(year,2) + \n               #language\n               telicity*language+ \n               caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\nsummary(mod16)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + finiteness + animacy + compound_tense + genre +  \n    control + language + caus_use + poly(year, 2) + telicity *  \n    poly(year, 2) + compound_tense * poly(year, 2) + genre *  \n    poly(year, 2) + control * poly(year, 2) + telicity * language +  \n    caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2608.8   2779.1  -1278.4   2556.8     5128 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-11.5964  -0.2205  -0.1168   0.2060   8.7160 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.2076   0.4557  \n lemma  (Intercept) 5.2234   2.2855  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        0.2072     0.7275   0.285  0.77574    \ntelicitytelic                     -2.2234     0.3916  -5.677 1.37e-08 ***\nfinitenessnonfin                   0.3900     0.1372   2.842  0.00449 ** \nanimacyyes                         0.8620     0.2064   4.177 2.95e-05 ***\ncompound_tenseyes                  0.4806     0.2817   1.706  0.08801 .  \ngenreProsa                        -0.4178     0.1706  -2.449  0.01434 *  \ngenreTeatro                       -0.4301     0.2815  -1.528  0.12657    \ncontrolyes                        -0.7841     0.2528  -3.102  0.00192 ** \nlanguageSpanish                    0.1607     0.9807   0.164  0.86984    \ncaus_use                           0.4973     0.7926   0.627  0.53036    \npoly(year, 2)1                    29.5471    13.3535   2.213  0.02692 *  \npoly(year, 2)2                     4.6329    11.5560   0.401  0.68849    \ntelicitytelic:poly(year, 2)1     -28.0188    10.1980  -2.747  0.00601 ** \ntelicitytelic:poly(year, 2)2     -24.9587     9.3513  -2.669  0.00761 ** \ncompound_tenseyes:poly(year, 2)1 -46.0537    21.1852  -2.174  0.02972 *  \ncompound_tenseyes:poly(year, 2)2 -21.3050    19.9558  -1.068  0.28570    \ngenreProsa:poly(year, 2)1        -31.2731    12.6468  -2.473  0.01341 *  \ngenreTeatro:poly(year, 2)1        -3.2607    23.3435  -0.140  0.88891    \ngenreProsa:poly(year, 2)2         22.4521    11.2151   2.002  0.04529 *  \ngenreTeatro:poly(year, 2)2        -8.1227    22.1365  -0.367  0.71367    \ncontrolyes:poly(year, 2)1         26.2792     9.8044   2.680  0.00735 ** \ncontrolyes:poly(year, 2)2        -21.2833     9.3111  -2.286  0.02227 *  \ntelicitytelic:languageSpanish      1.4715     0.4830   3.047  0.00232 ** \nlanguageSpanish:caus_use          -2.6181     1.0214  -2.563  0.01037 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod12, mod16) #not ok\n\nData: data\nModels:\nmod16: coding ~ telicity + finiteness + animacy + compound_tense + genre + control + language + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\nmod12: coding ~ telicity + animacy + finiteness + compound_tense + genre + control + language + reflpriming + caus_use + poly(year, 2) + telicity * poly(year, 2) + compound_tense * poly(year, 2) + genre * poly(year, 2) + control * poly(year, 2) + telicity * language + caus_use * language + (1 | lemma) + (1 | author)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmod16   26 2608.8 2779.1 -1278.4   2556.8                         \nmod12   27 2595.8 2772.6 -1270.9   2541.8 15.064  1  0.0001039 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe final model is mod12, which proves as the minimally adequate model\n\n\nModel report\nRefit mod12 after setting the reference level of the dependent variable data$coding to zero, and produce the reports and diagnostics of the model.\n\ndata$coding &lt;- relevel(data$coding, ref = \"zero\")  # Now \"antic\" is the level being predicted\n\nmod&lt;-glmer(coding ~ telicity + animacy + finiteness + compound_tense + genre + \n               control + language + reflpriming + caus_use + poly(year,2) +\n               #year\n               telicity*poly(year,2) + compound_tense*poly(year,2) + genre*poly(year,2) + \n               control*poly(year,2)  +\n               #language\n               telicity*language+ caus_use*language+\n               #random\n               (1|lemma) + (1|author), family=\"binomial\", data=data, nAGQ=0)\n\nsummary(mod)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: binomial  ( logit )\nFormula: coding ~ telicity + animacy + finiteness + compound_tense + genre +  \n    control + language + reflpriming + caus_use + poly(year,  \n    2) + telicity * poly(year, 2) + compound_tense * poly(year,  \n    2) + genre * poly(year, 2) + control * poly(year, 2) + telicity *  \n    language + caus_use * language + (1 | lemma) + (1 | author)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  2595.8   2772.6  -1270.9   2541.8     5127 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-9.9198 -0.2067  0.1148  0.2193 11.6137 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n author (Intercept) 0.1994   0.4466  \n lemma  (Intercept) 5.2094   2.2824  \nNumber of obs: 5154, groups:  author, 616; lemma, 40\n\nFixed effects:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       -0.2844     0.7271  -0.391 0.695712    \ntelicitytelic                      2.2402     0.3930   5.699 1.20e-08 ***\nanimacyyes                        -0.8563     0.2069  -4.140 3.48e-05 ***\nfinitenessnonfin                  -0.3961     0.1372  -2.887 0.003887 ** \ncompound_tenseyes                 -0.4842     0.2833  -1.709 0.087495 .  \ngenreProsa                         0.4371     0.1708   2.559 0.010490 *  \ngenreTeatro                        0.4871     0.2813   1.732 0.083352 .  \ncontrolyes                         0.7756     0.2533   3.062 0.002199 ** \nlanguageSpanish                   -0.1671     0.9793  -0.171 0.864520    \nreflprimingyes                     0.7960     0.2118   3.759 0.000171 ***\ncaus_use                          -0.5272     0.7913  -0.666 0.505245    \npoly(year, 2)1                   -29.4819    13.4208  -2.197 0.028039 *  \npoly(year, 2)2                    -6.4140    11.6002  -0.553 0.580316    \ntelicitytelic:poly(year, 2)1      29.9739    10.2595   2.922 0.003483 ** \ntelicitytelic:poly(year, 2)2      24.9774     9.3679   2.666 0.007670 ** \ncompound_tenseyes:poly(year, 2)1  43.9612    21.3327   2.061 0.039328 *  \ncompound_tenseyes:poly(year, 2)2  24.6607    19.9895   1.234 0.217321    \ngenreProsa:poly(year, 2)1         30.5676    12.6635   2.414 0.015786 *  \ngenreTeatro:poly(year, 2)1         0.3106    23.2091   0.013 0.989322    \ngenreProsa:poly(year, 2)2        -20.2971    11.2061  -1.811 0.070101 .  \ngenreTeatro:poly(year, 2)2        11.2673    22.1161   0.509 0.610428    \ncontrolyes:poly(year, 2)1        -27.1658     9.8018  -2.772 0.005580 ** \ncontrolyes:poly(year, 2)2         19.6613     9.3335   2.107 0.035159 *  \ntelicitytelic:languageSpanish     -1.4676     0.4840  -3.032 0.002429 ** \nlanguageSpanish:caus_use           2.5584     1.0196   2.509 0.012101 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nreport&lt;-list_report(mod, data$coding)\ndiagn&lt;-make_diagnostics(mod, data$coding)\n\n## ---------------------------------------------------------------------------------\ndata$pred_diac &lt;- predict(mod,type=\"response\")\ndata$pred_diac &lt;- ifelse(data$pred_diac&gt;0.5,\"anticaus\", \"zero\")\nprop.table(table(data$pred_diac,data$coding),1)*100\n\n          \n                zero     antic\n  anticaus  6.068189 93.931811\n  zero     89.473684 10.526316\n\npredmoddiac&lt;-as.data.frame(table(data$pred_diac, data$coding))\n\n\n## ---------------------------------------------------------------------------------\n\ndiagn %&gt;%\n  mutate_if(is.numeric, ~ round(.x, 2)) %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\nTest\nFit\n\n\n\n\nAIC\n2595.78\n\n\nBIC\n2772.56\n\n\nR$^2$ (conditional)\n0.67\n\n\nR$^2$ (marginal)\n0.12\n\n\nC\n0.97\n\n\nDxy\n0.93\n\n\nObservations\n5154.00\n\n\n\n\n## ---------------------------------------------------------------------------------\n\nreport$fixedsmall$LogOdds &lt;- log(report$fixedsmall$OR)\n\n\nreport$fixedsmall %&gt;% \n  mutate_if(is.numeric, ~ round(.x, 2)) %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\nVariable\nValue\nOR\np\nLogOdds\n\n\n\n\n\\textsc{ }\n(Intercept)\n7.500000e-01\n0.70\n-0.28\n\n\n\\textsc{Telicity}\ntelic\n9.390000e+00\n0.00***\n2.24\n\n\n\\textsc{Animacy}\nyes\n4.200000e-01\n0.00***\n-0.86\n\n\n\\textsc{Finiteness}\nnonfin\n6.700000e-01\n0.00**\n-0.40\n\n\n\\textsc{Compound Tense [Yes]}\nNA\n6.200000e-01\n0.09.\n-0.48\n\n\n\\textsc{Genre}\nProsa\n1.550000e+00\n0.01*\n0.44\n\n\n\\textsc{Genre}\nTeatro\n1.630000e+00\n0.08.\n0.49\n\n\n\\textsc{Control}\nyes\n2.170000e+00\n0.00**\n0.78\n\n\n\\textsc{Language}\nSpanish\n8.500000e-01\n0.86\n-0.17\n\n\n\\textsc{Reflpriming}\nyes\n2.220000e+00\n0.00***\n0.80\n\n\n\\textsc{Caus Use}\nN words\n5.900000e-01\n0.51\n-0.53\n\n\n\\textsc{Year}\n1st degree\n0.000000e+00\n0.03*\n-29.48\n\n\n\\textsc{Year}\n2nd degree\n0.000000e+00\n0.58\n-6.41\n\n\n\\textsc{Telicity}\ntelic × year [1st degree]\n1.041106e+13\n0.00**\n29.97\n\n\n\\textsc{Telicity}\ntelic × year [2nd degree]\n7.039504e+10\n0.01**\n24.98\n\n\n\\textsc{Compound Tense [Yes] × Year [1st Degree]}\nNA\n1.236238e+19\n0.04*\n43.96\n\n\n\\textsc{Compound Tense [Yes] × Year [2nd Degree]}\nNA\n5.128566e+10\n0.22\n24.66\n\n\n\\textsc{Genre}\nProsa × year [1st degree]\n1.885190e+13\n0.02*\n30.57\n\n\n\\textsc{Genre}\nTeatro × year [1st degree]\n1.360000e+00\n0.99\n0.31\n\n\n\\textsc{Genre}\nProsa × year [2nd degree]\n0.000000e+00\n0.07.\n-20.30\n\n\n\\textsc{Genre}\nTeatro × year [2nd degree]\n7.822168e+04\n0.61\n11.27\n\n\n\\textsc{Control}\nyes × year [1st degree]\n0.000000e+00\n0.01**\n-27.17\n\n\n\\textsc{Control}\nyes × year [2nd degree]\n3.457896e+08\n0.04*\n19.66\n\n\n\\textsc{Telicity}\ntelic × language [Spanish]\n2.300000e-01\n0.00**\n-1.47\n\n\n\\textsc{Language}\nSpanish × caus use\n1.292000e+01\n0.01*\n2.56\n\n\n\n\n\n\n\nModel Visualisation\n\nRandom effects\nFigure 4:\n\nranef_data &lt;- lme4::ranef(mod)[[\"lemma\"]]\n\nnames &lt;- rownames(ranef_data)\nrownames(ranef_data) &lt;- NULL\nranef_data &lt;- cbind(names,ranef_data)\n\nranefdata2 &lt;-\n  left_join(ranef_data,\n            data %&gt;% dplyr::select(lemma, language)%&gt;% rename(names=lemma) %&gt;% unique,\n            by = \"names\")\n\nranefdata2&lt;- ranefdata2 %&gt;% \n  rename(Intercept = `(Intercept)`, Variable= names) %&gt;% \n  mutate(color = ifelse(language == \"italian\", \"red\", \"#f1947a\"))\n\ncolors &lt;- ranefdata2$color[order(ranefdata2$Intercept)]\n\nranefdata2 %&gt;% \n  ggplot(aes(y = Intercept, x = reorder(Variable, Intercept), fill=language, pattern=Intercept&gt;0)) + \n  geom_bar_pattern(stat=\"identity\",\n                   position = position_dodge(0.9), \n                   pattern_fill = \"black\",\n                   pattern_angle = 45,\n                   pattern_density = 0.1,\n                   pattern_spacing = 0.025,\n                   pattern_key_scale_factor = 0.6,\n                   alpha=0.8,\n                   colour=\"#e9ecef\") +\n  scale_fill_manual(name = \"Language\", values = setNames(c(\"red\", \"#f1947a\"),c(\"italian\", \"spanish\")), labels = c( \"italian\", \"spanish\"))+\n  scale_pattern_manual(values = c(\"none\", \"stripe\"), labels=c(\"zero\", \"anticausative\"))+\n  labs(y = \"Log-odds\", x=\"LEMMA\", pattern= \"Coding pattern\") +\n  coord_flip() +\n  theme_minimal(base_size = 9)+\n  guides(pattern = guide_legend(override.aes = list(fill = \"grey\")),\n         fill = guide_legend(override.aes = list(pattern = \"none\")))+\n  theme(axis.text.y = element_text(colour= colors))\n\n\n\n\n\n\n\n\n\n\nSimple effects\nFigure 5:\n\neffects_list2 &lt;- purrr::map(c( \"finiteness\",\"reflpriming\", \"animacy\"),\n                           ~simpleff(mod, .x) + \n    theme_minimal(base_size = 9, base_family = \"cambria\")+\n    labs(x=\"\", y=\"\")+\n    theme(text = element_text(colour = \"black\"),\n          axis.text = element_text(colour = \"black\")\n    )\n)\n\ncowplot::plot_grid(plotlist = effects_list2,  align = \"v\")\n\n\n\n\n\n\n\n\nFigure 6:\n\nyear_eff&lt;-makeff(mod, \"year\")\nyearplot&lt;-ggplot(year_eff, aes(x=x, y=predicted, group=1)) +\n  geom_line(size=0.7, color=safe[1])+\n  labs(x= \"Real time\", y=\"\")+\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6), limits = c(0,1), labels = scales::percent)+\n  theme_minimal(base_size = 9)\n\nyearplot&lt;-yearplot+ggtitle(\"YEAR\")\nyearplot +  theme_minimal(base_size = 15) + geom_line(size=1.2, color=safe[1])\n\n\n\n\n\n\n\n\n\n\nInteractions\nFigure 7:\n\ndiacsmooth(mod, var=c(\"year\", \"compound_tense\"))\n\n\n\n\n\n\n\n\nFigure 8:\n\ndiacsmooth(mod, var=c(\"year\", \"control\"))\n\n\n\n\n\n\n\n\nFigure 9:\n\ndiacsmooth(mod, var=c(\"year\", \"telicity\"))\n\n\n\n\n\n\n\n\nFigure 10\n\ndiacsmooth(mod, var=c(\"year\", \"genre\"))\n\n\n\n\n\n\n\n\nFigure 11:\n\ninterplot(mod, var=c(\"telicity\", \"language\"))\n\n\n\n\n\n\n\n\nFigure 12:\n\ndf&lt;-table(data$azion, data$coding, data$language) %&gt;% as.data.frame()\ncolnames(df)&lt;-c(\"var\", \"altern\", \"language\", \"freq\")\ndf$var&lt;-df$var %&gt;%  fct_relevel(c(\"accomplishment\", \"achievement\", \"degree\", \"activity\"))\n \n ggplot(df, aes(y=freq, x=var, fill=altern)) +\n     geom_bar(position=\"fill\", stat=\"identity\", alpha=0.8)+\n     scale_y_continuous(labels = scales::percent)+\n     theme(legend.title = element_blank())+\n     labs (x=\"\", y= \"\")+ coord_flip()+\n     facet_wrap(vars(language), ncol = 5)+\n     scale_fill_manual(values=safe[c(2,1)])\n\n\n\n\n\n\n\n\nFigure 13:\n\ndiacsmooth(mod, var=c(\"caus_use\", \"language\"))+xlab(\"Causal %\")\n\n\n\n\n\n\n\n\nFigure 14:\n\ncaus_eff&lt;-makeff(mod, \"caus_use\")\n\ncausplot&lt;-ggplot(caus_eff, aes(x=x, y=predicted, group=1)) +\n  geom_line(size=1.2, color=safe[1])+\n  labs(title= \"CAUSALNESS DEGREE\", x=\"Causalness degree\", y=\"\")+\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6), limits = c(0,1), labels = scales::percent)+\n  theme_minimal(base_size = 9)\n\ncausplot +  theme_minimal(base_size = 15) + geom_line(size=1.2, color=safe[1])"
  }
]